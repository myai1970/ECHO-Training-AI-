{"cells":[{"cell_type":"markdown","metadata":{"id":"f9IwuLMIQYGK"},"source":["# Introduction to HuggingFace Transformers\n","\n","**DOST-ITDI AI Training Workshop**\n","\n","Day 1, Session 4: Deep Learning with PyTorch and HuggingFace\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"xrkc4LJQQYGP"},"source":["## What is a Transformer?\n","\n","### The Problem: Understanding Context\n","\n","Imagine reading this sentence:\n","\n","> \"The **bank** was flooded after the heavy rain.\"\n","\n","Is \"bank\" a:\n","- Financial institution (BDO, BPI)?\n","- River bank?\n","\n","You know it's a **river bank** because of the words \"flooded\" and \"rain\".\n","\n","**Transformers do the same thing** - they look at ALL words in a sentence to understand the meaning of each word.\n","\n","---\n","\n","### The Key Idea: Attention\n","\n","Think of it like asking questions:\n","\n","```\n","Word: \"bank\"\n","Question: \"What other words help me understand 'bank'?\"\n","\n","Answer:\n","  - \"flooded\" -> Very relevant! (high attention)\n","  - \"rain\"    -> Relevant (high attention)\n","  - \"The\"     -> Not helpful (low attention)\n","  - \"was\"     -> Not helpful (low attention)\n","```\n","\n","The model learns to **pay attention** to the right words.\n","\n","---\n","\n","### Simple Analogy: Group Discussion\n","\n","Imagine a classroom discussion:\n","\n","1. **Old way (RNN)**: Students pass notes one-by-one in a line\n","   - Slow, information gets lost\n","   - Student 10 barely remembers what Student 1 said\n","\n","2. **Transformer way**: Everyone can talk to everyone directly\n","   - Fast, parallel processing\n","   - Each student decides who to listen to\n","\n","---\n","\n","### Why \"Transformer\"?\n","\n","It **transforms** each word's representation by mixing in information from other relevant words.\n","\n","```\n","Input:  [The] [bank] [was] [flooded]\n","           |     |      |      |\n","        (each word looks at all others)\n","           |     |      |      |\n","Output: [The] [river bank] [was] [flooded]\n","              (now enriched with context!)\n","```"]},{"cell_type":"markdown","metadata":{"id":"btEuqU4kQYGS"},"source":["## Famous Transformer Models\n","\n","| Model | What it does | Analogy |\n","|-------|--------------|--------|\n","| **BERT** | Understands text | Reading comprehension expert |\n","| **GPT** | Generates text | Creative writer |\n","| **T5** | Text-to-text | Translator/Summarizer |\n","\n","### For Science:\n","| Model | Domain |\n","|-------|--------|\n","| SciBERT | Scientific papers |\n","| ChemBERTa | Chemistry/Molecules |\n","| BioBERT | Biomedical text |"]},{"cell_type":"markdown","metadata":{"id":"ze0sJkwUQYGT"},"source":["---\n","\n","## Let's Use HuggingFace!\n","\n","HuggingFace makes it easy to use these powerful models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PdDqDEv4QYGU"},"outputs":[],"source":["# Install\n","!pip install transformers torch -q\n","print(\"Ready!\")"]},{"cell_type":"markdown","metadata":{"id":"CJtOaLK7QYGW"},"source":["## 1. Sentiment Analysis\n","\n","Is this text positive or negative?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tw6Hh8QUQYGX"},"outputs":[],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\")\n","\n","texts = [\n","    \"The experiment was a complete success!\",\n","    \"The reaction failed and we lost all samples.\",\n","    \"Results are still being analyzed.\"\n","]\n","\n","for text in texts:\n","    result = classifier(text)[0]\n","    print(f\"{result['label']}: {text}\")"]},{"cell_type":"markdown","metadata":{"id":"M5KcF2WQQYGY"},"source":["## 2. Named Entity Recognition (NER)\n","\n","Find names, places, organizations in text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1oFdK1XaQYGa"},"outputs":[],"source":["from transformers import pipeline\n","\n","ner = pipeline(\"ner\", grouped_entities=True)\n","\n","text = \"Dr. Santos from DOST-ITDI published a study on paracetamol synthesis in Manila.\"\n","\n","print(f\"Text: {text}\\n\")\n","print(\"Entities found:\")\n","\n","for entity in ner(text):\n","    print(f\"  {entity['word']:20} -> {entity['entity_group']}\")"]},{"cell_type":"markdown","metadata":{"id":"bN_nocuFQYGc"},"source":["## 3. Question Answering\n","\n","Ask questions about a paragraph."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRgISMO6QYGd"},"outputs":[],"source":["from transformers import pipeline\n","\n","qa = pipeline(\"question-answering\")\n","\n","context = \"\"\"\n","Paracetamol, also known as acetaminophen, is a medication used to treat pain and fever.\n","It was first synthesized in 1877. The molecular weight is 151.16 g/mol.\n","It is one of the most commonly used medications worldwide.\n","\"\"\"\n","\n","questions = [\n","    \"What is paracetamol used for?\",\n","    \"When was it first synthesized?\",\n","    \"What is the molecular weight?\"\n","]\n","\n","for q in questions:\n","    answer = qa(question=q, context=context)\n","    print(f\"Q: {q}\")\n","    print(f\"A: {answer['answer']}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"DDODon8KQYGf"},"source":["## 4. Zero-Shot Classification\n","\n","Classify text WITHOUT any training data!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XoXzA8QwQYGf"},"outputs":[],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"zero-shot-classification\")\n","\n","texts = [\n","    \"The solution turned blue after adding copper sulfate\",\n","    \"Patient showed improvement after 3 days of treatment\",\n","    \"The algorithm converged after 100 iterations\"\n","]\n","\n","labels = [\"chemistry\", \"medicine\", \"computer science\"]\n","\n","for text in texts:\n","    result = classifier(text, labels)\n","    print(f\"{result['labels'][0]:20} <- {text}\")"]},{"cell_type":"markdown","metadata":{"id":"54LPnJFbQYGg"},"source":["## 5. Text Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aw4D-KKVQYGg"},"outputs":[],"source":["from transformers import pipeline\n","\n","generator = pipeline(\"text-generation\", model=\"distilgpt2\", max_length=40)\n","\n","prompt = \"In chemistry, a catalyst is\"\n","\n","result = generator(prompt, num_return_sequences=1)\n","print(result[0]['generated_text'])"]},{"cell_type":"markdown","metadata":{"id":"60b3jyzVQYGh"},"source":["## 6. ChemBERTa: Understanding Molecules\n","\n","ChemBERTa can read SMILES (molecular text) and understand molecular structure!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvSVDekyQYGi"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","import numpy as np\n","\n","# Load ChemBERTa\n","tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n","model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n","\n","molecules = {\n","    \"Ethanol\": \"CCO\",\n","    \"Methanol\": \"CO\",\n","    \"Benzene\": \"c1ccccc1\",\n","    \"Toluene\": \"Cc1ccccc1\"\n","}\n","\n","# Get embeddings\n","embeddings = {}\n","for name, smiles in molecules.items():\n","    tokens = tokenizer(smiles, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        output = model(**tokens)\n","    embeddings[name] = output.last_hidden_state.mean(dim=1).numpy()[0]\n","    print(f\"{name:10} ({smiles:12}) -> embedding created\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wZ0wBtkQYGi"},"outputs":[],"source":["# Compare molecular similarity\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","names = list(embeddings.keys())\n","emb_matrix = np.array([embeddings[n] for n in names])\n","sim = cosine_similarity(emb_matrix)\n","\n","print(\"Molecular Similarity:\\n\")\n","print(f\"{'':12}\", end=\"\")\n","for n in names:\n","    print(f\"{n:12}\", end=\"\")\n","print()\n","\n","for i, n1 in enumerate(names):\n","    print(f\"{n1:12}\", end=\"\")\n","    for j in range(len(names)):\n","        print(f\"{sim[i,j]:12.2f}\", end=\"\")\n","    print()\n","\n","print(\"\\nNotice: Ethanol-Methanol are similar (alcohols)\")\n","print(\"        Benzene-Toluene are similar (aromatics)\")"]},{"cell_type":"markdown","metadata":{"id":"OYjEgxP4QYGi"},"source":["## Summary\n","\n","### What is Attention/Transformer?\n","- A way for models to understand context by looking at ALL words\n","- Each word \"pays attention\" to relevant words\n","- Much better than older sequential methods\n","\n","### HuggingFace Pipelines\n","\n","```python\n","from transformers import pipeline\n","\n","# Just pick your task!\n","pipe = pipeline(\"sentiment-analysis\")  \n","pipe = pipeline(\"ner\")\n","pipe = pipeline(\"question-answering\")\n","pipe = pipeline(\"zero-shot-classification\")\n","pipe = pipeline(\"text-generation\")\n","```\n","\n","### Scientific Models\n","- **SciBERT**: Scientific text\n","- **ChemBERTa**: SMILES/molecules  \n","- **BioBERT**: Biomedical\n","\n","### Resources\n","- https://huggingface.co/models\n","- https://huggingface.co/learn/nlp-course"]},{"cell_type":"markdown","metadata":{"id":"ut9cB-P3QYGj"},"source":["## Exercise\n","\n","1. Try sentiment analysis on your own research abstracts\n","2. Use NER to extract entities from a paper\n","3. Compare similarity of molecules in your research"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AOK9trGGQYGj"},"outputs":[],"source":["# Your code here\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.0"},"colab":{"provenance":[{"file_id":"https://github.com/jomaminoza/dost-ai-training/blob/main/notebooks/04b_HuggingFace_Intro.ipynb","timestamp":1764396999176}]}},"nbformat":4,"nbformat_minor":0}