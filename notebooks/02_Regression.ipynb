{"cells":[{"cell_type":"markdown","metadata":{"id":"Pz5W4mNEIIm3"},"source":["# Machine Learning Regression Models\n","\n","**DOST-ITDI AI Training Workshop**  \n","**Day 1 - Session 3: Machine Learning with Scikit-learn**\n","\n","---\n","\n","## Learning Objectives\n","1. Understand regression problems in chemistry\n","2. Implement various regression algorithms\n","3. Evaluate model performance using metrics\n","4. Compare different models\n","5. Apply feature importance analysis\n","\n","## What is Regression?\n","\n","Regression is a supervised learning technique used to predict **continuous values**.\n","\n","**Chemistry Applications**:\n","- Predicting molecular properties (boiling point, solubility, toxicity)\n","- Estimating reaction yields\n","- Forecasting material properties\n","- QSAR (Quantitative Structure-Activity Relationship) modeling"]},{"cell_type":"markdown","metadata":{"id":"OJyJ26OtIIm7"},"source":["## 1. Setup and Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drtHztL7IIm7"},"outputs":[],"source":["# Install required libraries\n","!pip install rdkit scikit-learn seaborn -q\n","\n","print(\"âœ“ Installation complete!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FhBD4fbgIIm-"},"outputs":[],"source":["# Import libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from rdkit import Chem\n","from rdkit.Chem import Descriptors\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Machine Learning\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.svm import SVR\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","\n","# Plotting style\n","sns.set_style(\"whitegrid\")\n","plt.rcParams['figure.figsize'] = (10, 6)\n","\n","print(\"âœ“ Libraries imported successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AxzQ3ZmIIm_"},"outputs":[],"source":["# Load ESOL dataset\n","url = \"https://raw.githubusercontent.com/deepchem/deepchem/master/datasets/delaney-processed.csv\"\n","df = pd.read_csv(url)\n","\n","print(f\"Dataset shape: {df.shape}\")\n","print(f\"\\nTarget variable: measured log solubility in mols per litre\")\n","print(f\"Range: {df['measured log solubility in mols per litre'].min():.2f} to {df['measured log solubility in mols per litre'].max():.2f}\")\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"rO-CFnEiIInA"},"source":["## 2. Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0CdkMs-ZIInB"},"outputs":[],"source":["# Compute additional descriptors\n","df['mol'] = df['smiles'].apply(Chem.MolFromSmiles)\n","df = df[df['mol'].notna()].copy()  # Remove invalid SMILES\n","\n","df['LogP'] = df['mol'].apply(Descriptors.MolLogP)\n","df['NumHAcceptors'] = df['mol'].apply(Descriptors.NumHAcceptors)\n","df['NumAromaticRings'] = df['mol'].apply(Descriptors.NumAromaticRings)\n","df['TPSA'] = df['mol'].apply(Descriptors.TPSA)\n","\n","print(f\"Dataset after processing: {df.shape[0]} molecules\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3H_EtwDIInC"},"outputs":[],"source":["# Select features\n","feature_columns = [\n","    'Molecular Weight',\n","    'Number of H-Bond Donors',\n","    'Number of Rings',\n","    'Number of Rotatable Bonds',\n","    'Polar Surface Area',\n","    'LogP',\n","    'NumHAcceptors',\n","    'NumAromaticRings'\n","]\n","\n","X = df[feature_columns]\n","y = df['measured log solubility in mols per litre']\n","\n","print(\"Features:\")\n","print(X.head())\n","print(f\"\\nFeatures shape: {X.shape}\")\n","print(f\"Target shape: {y.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B94-9uHPIInE"},"outputs":[],"source":["# Split data: 80% training, 20% testing\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","print(\"Dataset Split:\")\n","print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n","print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lt48VXoGIInG"},"outputs":[],"source":["# Feature scaling (standardization)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","print(\"Feature scaling complete!\")\n","print(f\"Mean of scaled training features: {X_train_scaled.mean(axis=0).round(10)}\")\n","print(f\"Std of scaled training features: {X_train_scaled.std(axis=0).round(2)}\")"]},{"cell_type":"markdown","metadata":{"id":"dmf0VbF2IInI"},"source":["## 3. Regression Models\n","\n","### 3.1 Linear Regression\n","\n","The simplest regression model: finds the best-fit line through the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TVwpFU7GIInJ"},"outputs":[],"source":["# Train Linear Regression\n","lr = LinearRegression()\n","lr.fit(X_train_scaled, y_train)\n","\n","# Make predictions\n","y_train_pred_lr = lr.predict(X_train_scaled)\n","y_test_pred_lr = lr.predict(X_test_scaled)\n","\n","# Evaluate\n","print(\"Linear Regression Results:\")\n","print(f\"Training RÂ²: {r2_score(y_train, y_train_pred_lr):.4f}\")\n","print(f\"Test RÂ²: {r2_score(y_test, y_test_pred_lr):.4f}\")\n","print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred_lr)):.4f}\")\n","print(f\"Test MAE: {mean_absolute_error(y_test, y_test_pred_lr):.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8PEDGE40IInK"},"outputs":[],"source":["# Feature importance (coefficients)\n","feature_importance = pd.DataFrame({\n","    'Feature': feature_columns,\n","    'Coefficient': lr.coef_\n","}).sort_values('Coefficient', key=abs, ascending=False)\n","\n","plt.figure(figsize=(10, 6))\n","colors = ['red' if x < 0 else 'green' for x in feature_importance['Coefficient']]\n","plt.barh(feature_importance['Feature'], feature_importance['Coefficient'],\n","         color=colors, alpha=0.7, edgecolor='black')\n","plt.xlabel('Coefficient Value', fontsize=12)\n","plt.title('Linear Regression: Feature Importance', fontsize=14, fontweight='bold')\n","plt.axvline(0, color='black', linewidth=0.8)\n","plt.grid(True, alpha=0.3, axis='x')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"a1KO5IWuIInL"},"source":["### 3.2 Ridge Regression (L2 Regularization)\n","\n","Prevents overfitting by penalizing large coefficients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcNmlrPoIInL"},"outputs":[],"source":["# Train Ridge Regression\n","ridge = Ridge(alpha=1.0, random_state=42)\n","ridge.fit(X_train_scaled, y_train)\n","\n","# Predictions\n","y_train_pred_ridge = ridge.predict(X_train_scaled)\n","y_test_pred_ridge = ridge.predict(X_test_scaled)\n","\n","# Evaluate\n","print(\"Ridge Regression Results:\")\n","print(f\"Training RÂ²: {r2_score(y_train, y_train_pred_ridge):.4f}\")\n","print(f\"Test RÂ²: {r2_score(y_test, y_test_pred_ridge):.4f}\")\n","print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred_ridge)):.4f}\")\n","print(f\"Test MAE: {mean_absolute_error(y_test, y_test_pred_ridge):.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"wapVm2gcIInM"},"source":["### 3.3 Random Forest Regression\n","\n","Ensemble of decision trees - powerful and versatile."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3k6ohgCIInM"},"outputs":[],"source":["# Train Random Forest\n","rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n","rf.fit(X_train_scaled, y_train)\n","\n","# Predictions\n","y_train_pred_rf = rf.predict(X_train_scaled)\n","y_test_pred_rf = rf.predict(X_test_scaled)\n","\n","# Evaluate\n","print(\"Random Forest Results:\")\n","print(f\"Training RÂ²: {r2_score(y_train, y_train_pred_rf):.4f}\")\n","print(f\"Test RÂ²: {r2_score(y_test, y_test_pred_rf):.4f}\")\n","print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred_rf)):.4f}\")\n","print(f\"Test MAE: {mean_absolute_error(y_test, y_test_pred_rf):.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kqPK-7AHIInN"},"outputs":[],"source":["# Random Forest Feature Importance\n","rf_importance = pd.DataFrame({\n","    'Feature': feature_columns,\n","    'Importance': rf.feature_importances_\n","}).sort_values('Importance', ascending=False)\n","\n","plt.figure(figsize=(10, 6))\n","plt.barh(rf_importance['Feature'], rf_importance['Importance'],\n","         color='steelblue', alpha=0.7, edgecolor='black')\n","plt.xlabel('Importance Score', fontsize=12)\n","plt.title('Random Forest: Feature Importance', fontsize=14, fontweight='bold')\n","plt.grid(True, alpha=0.3, axis='x')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"n2Xsoy46IInN"},"source":["### 3.4 Gradient Boosting Regression\n","\n","Sequential ensemble method - builds trees to correct previous errors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkkXXAfHIInO"},"outputs":[],"source":["# Train Gradient Boosting\n","gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n","                                 max_depth=3, random_state=42)\n","gbr.fit(X_train_scaled, y_train)\n","\n","# Predictions\n","y_train_pred_gbr = gbr.predict(X_train_scaled)\n","y_test_pred_gbr = gbr.predict(X_test_scaled)\n","\n","# Evaluate\n","print(\"Gradient Boosting Results:\")\n","print(f\"Training RÂ²: {r2_score(y_train, y_train_pred_gbr):.4f}\")\n","print(f\"Test RÂ²: {r2_score(y_test, y_test_pred_gbr):.4f}\")\n","print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred_gbr)):.4f}\")\n","print(f\"Test MAE: {mean_absolute_error(y_test, y_test_pred_gbr):.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"7RTM65rMIInO"},"source":["## 4. Model Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlxdCNOQIInP"},"outputs":[],"source":["# Compile results\n","results = pd.DataFrame({\n","    'Model': ['Linear Regression', 'Ridge', 'Random Forest', 'Gradient Boosting'],\n","    'Train RÂ²': [\n","        r2_score(y_train, y_train_pred_lr),\n","        r2_score(y_train, y_train_pred_ridge),\n","        r2_score(y_train, y_train_pred_rf),\n","        r2_score(y_train, y_train_pred_gbr)\n","    ],\n","    'Test RÂ²': [\n","        r2_score(y_test, y_test_pred_lr),\n","        r2_score(y_test, y_test_pred_ridge),\n","        r2_score(y_test, y_test_pred_rf),\n","        r2_score(y_test, y_test_pred_gbr)\n","    ],\n","    'Test RMSE': [\n","        np.sqrt(mean_squared_error(y_test, y_test_pred_lr)),\n","        np.sqrt(mean_squared_error(y_test, y_test_pred_ridge)),\n","        np.sqrt(mean_squared_error(y_test, y_test_pred_rf)),\n","        np.sqrt(mean_squared_error(y_test, y_test_pred_gbr))\n","    ],\n","    'Test MAE': [\n","        mean_absolute_error(y_test, y_test_pred_lr),\n","        mean_absolute_error(y_test, y_test_pred_ridge),\n","        mean_absolute_error(y_test, y_test_pred_rf),\n","        mean_absolute_error(y_test, y_test_pred_gbr)\n","    ]\n","})\n","\n","print(\"\\nModel Performance Comparison:\")\n","print(results.to_string(index=False))\n","\n","# Find best model\n","best_model_idx = results['Test RÂ²'].idxmax()\n","print(f\"\\nðŸ† Best Model: {results.loc[best_model_idx, 'Model']}\")\n","print(f\"   Test RÂ²: {results.loc[best_model_idx, 'Test RÂ²']:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_zJmapLIInQ"},"outputs":[],"source":["# Visualize model comparison\n","fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","\n","# RÂ² Score comparison\n","x = np.arange(len(results))\n","width = 0.35\n","\n","axes[0].bar(x - width/2, results['Train RÂ²'], width, label='Train',\n","            alpha=0.8, edgecolor='black')\n","axes[0].bar(x + width/2, results['Test RÂ²'], width, label='Test',\n","            alpha=0.8, edgecolor='black')\n","axes[0].set_xlabel('Model', fontsize=12)\n","axes[0].set_ylabel('RÂ² Score', fontsize=12)\n","axes[0].set_title('RÂ² Score Comparison', fontsize=13, fontweight='bold')\n","axes[0].set_xticks(x)\n","axes[0].set_xticklabels(results['Model'], rotation=45, ha='right')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3, axis='y')\n","\n","# RMSE comparison\n","axes[1].bar(results['Model'], results['Test RMSE'],\n","            color='coral', alpha=0.8, edgecolor='black')\n","axes[1].set_xlabel('Model', fontsize=12)\n","axes[1].set_ylabel('RMSE', fontsize=12)\n","axes[1].set_title('Test RMSE Comparison (Lower is Better)', fontsize=13, fontweight='bold')\n","axes[1].set_xticklabels(results['Model'], rotation=45, ha='right')\n","axes[1].grid(True, alpha=0.3, axis='y')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"KzVHQvfwIInQ"},"source":["## 5. Prediction Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tglaW_vZIInR"},"outputs":[],"source":["# Actual vs Predicted plots\n","fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n","axes = axes.ravel()\n","\n","predictions = [\n","    ('Linear Regression', y_test_pred_lr),\n","    ('Ridge', y_test_pred_ridge),\n","    ('Random Forest', y_test_pred_rf),\n","    ('Gradient Boosting', y_test_pred_gbr)\n","]\n","\n","for idx, (name, y_pred) in enumerate(predictions):\n","    axes[idx].scatter(y_test, y_pred, alpha=0.5, s=50, edgecolors='black', linewidth=0.5)\n","\n","    # Perfect prediction line\n","    min_val = min(y_test.min(), y_pred.min())\n","    max_val = max(y_test.max(), y_pred.max())\n","    axes[idx].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n","\n","    axes[idx].set_xlabel('Actual Log Solubility', fontsize=11)\n","    axes[idx].set_ylabel('Predicted Log Solubility', fontsize=11)\n","    axes[idx].set_title(f'{name}', fontsize=12, fontweight='bold')\n","    axes[idx].grid(True, alpha=0.3)\n","    axes[idx].legend()\n","\n","    # Add RÂ² score\n","    r2 = r2_score(y_test, y_pred)\n","    axes[idx].text(0.05, 0.95, f'RÂ² = {r2:.4f}', transform=axes[idx].transAxes,\n","                  fontsize=10, verticalalignment='top',\n","                  bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"V3gYk6HrIInR"},"source":["## 6. Residual Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DG8soURmIInS"},"outputs":[],"source":["# Residuals = Actual - Predicted\n","residuals_rf = y_test - y_test_pred_rf\n","\n","fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","\n","# Residual plot\n","axes[0].scatter(y_test_pred_rf, residuals_rf, alpha=0.5, s=50,\n","                edgecolors='black', linewidth=0.5)\n","axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n","axes[0].set_xlabel('Predicted Log Solubility', fontsize=12)\n","axes[0].set_ylabel('Residuals', fontsize=12)\n","axes[0].set_title('Residual Plot (Random Forest)', fontsize=13, fontweight='bold')\n","axes[0].grid(True, alpha=0.3)\n","\n","# Residual distribution\n","axes[1].hist(residuals_rf, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n","axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n","axes[1].set_xlabel('Residuals', fontsize=12)\n","axes[1].set_ylabel('Frequency', fontsize=12)\n","axes[1].set_title('Distribution of Residuals', fontsize=13, fontweight='bold')\n","axes[1].grid(True, alpha=0.3, axis='y')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"Mean Residual: {residuals_rf.mean():.6f}\")\n","print(f\"Std Dev of Residuals: {residuals_rf.std():.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"rAxNAiesIInT"},"source":["## 7. Cross-Validation\n","\n","More robust evaluation using multiple train/test splits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kg1E2MdaIInT"},"outputs":[],"source":["# 5-Fold Cross-Validation\n","models = {\n","    'Linear Regression': LinearRegression(),\n","    'Ridge': Ridge(alpha=1.0),\n","    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n","    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n","}\n","\n","cv_results = {}\n","\n","print(\"Cross-Validation Results (5-Fold):\")\n","print(\"=\" * 60)\n","\n","for name, model in models.items():\n","    scores = cross_val_score(model, X_train_scaled, y_train,\n","                            cv=5, scoring='r2', n_jobs=-1)\n","    cv_results[name] = scores\n","    print(f\"{name:20} | Mean RÂ²: {scores.mean():.4f} (+/- {scores.std():.4f})\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_UFJnbfIInT"},"outputs":[],"source":["# Visualize cross-validation results\n","plt.figure(figsize=(10, 6))\n","plt.boxplot(cv_results.values(), labels=cv_results.keys())\n","plt.ylabel('RÂ² Score', fontsize=12)\n","plt.title('Cross-Validation RÂ² Scores (5-Fold)', fontsize=14, fontweight='bold')\n","plt.xticks(rotation=45, ha='right')\n","plt.grid(True, alpha=0.3, axis='y')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"TS-1Q5FVIInU"},"source":["## 8. Making Predictions on New Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uD_WelJjIInU"},"outputs":[],"source":["# Example: Predict solubility for a new molecule\n","# Let's use the best model (Random Forest)\n","\n","# Take a few examples from test set\n","sample_indices = [0, 10, 20, 30, 40]\n","samples = X_test.iloc[sample_indices]\n","samples_scaled = scaler.transform(samples)\n","\n","# Predict\n","predictions = rf.predict(samples_scaled)\n","actual = y_test.iloc[sample_indices].values\n","\n","# Display results\n","results_df = pd.DataFrame({\n","    'Actual': actual,\n","    'Predicted': predictions,\n","    'Error': actual - predictions,\n","    'Abs Error': np.abs(actual - predictions)\n","})\n","\n","print(\"\\nSample Predictions:\")\n","print(results_df.to_string())\n","print(f\"\\nMean Absolute Error: {results_df['Abs Error'].mean():.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"y_PTG-urIInV"},"source":["## 9. Understanding Regression Metrics\n","\n","### Key Metrics:\n","\n","1. **RÂ² Score (Coefficient of Determination)**\n","   - Range: -âˆž to 1 (1 is perfect)\n","   - Proportion of variance explained by the model\n","   - Higher is better\n","\n","2. **RMSE (Root Mean Squared Error)**\n","   - Same units as target variable\n","   - Penalizes large errors more\n","   - Lower is better\n","\n","3. **MAE (Mean Absolute Error)**\n","   - Average absolute error\n","   - More robust to outliers than RMSE\n","   - Lower is better"]},{"cell_type":"markdown","metadata":{"id":"1uyjTPZ4IInV"},"source":["## 10. Summary and Key Takeaways\n","\n","### What We Learned:\n","\n","1. **Regression Problem Setup**\n","   - Feature selection and engineering\n","   - Data splitting (train/test)\n","   - Feature scaling\n","\n","2. **Multiple Regression Models**\n","   - Linear Regression (baseline)\n","   - Ridge (regularized linear)\n","   - Random Forest (ensemble)\n","   - Gradient Boosting (sequential ensemble)\n","\n","3. **Model Evaluation**\n","   - RÂ², RMSE, MAE metrics\n","   - Cross-validation\n","   - Residual analysis\n","\n","4. **Best Practices**\n","   - Always use train/test split\n","   - Scale features for linear models\n","   - Compare multiple models\n","   - Use cross-validation for robust evaluation\n","   - Analyze residuals to check model assumptions\n","\n","### Chemistry Insights:\n","- LogP (lipophilicity) is a strong predictor of solubility\n","- Molecular weight negatively correlates with solubility\n","- Ensemble methods (RF, GBR) often outperform linear models\n","- Feature importance reveals key molecular properties"]},{"cell_type":"markdown","metadata":{"id":"Rq7mqn1tIInV"},"source":["## 11. Exercise: Build Your Own Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLxqSbMpIInW"},"outputs":[],"source":["# TODO: Try different regression models or parameters\n","# 1. Try SVR (Support Vector Regression)\n","# 2. Try different Random Forest parameters (n_estimators, max_depth)\n","# 3. Try XGBoost if installed\n","# 4. Compare your results with the models above\n","\n","# Your code here:\n"]},{"cell_type":"markdown","metadata":{"id":"ULQQnURKIInX"},"source":["---\n","\n","## Resources\n","\n","- [Scikit-learn Regression Guide](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n","- [Understanding RÂ² Score](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n","- [QSAR Modeling](https://en.wikipedia.org/wiki/Quantitative_structure%E2%80%93activity_relationship)\n","\n","**Next Notebook: Classification Models for Molecular Data**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[{"file_id":"https://github.com/jomaminoza/dost-ai-training/blob/main/notebooks/02_Regression.ipynb","timestamp":1764394837658}]}},"nbformat":4,"nbformat_minor":0}