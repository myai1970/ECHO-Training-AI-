{"cells":[{"cell_type":"markdown","metadata":{"id":"vyHjlpuURihN"},"source":["# Computer Vision for Scientific Research\n","\n","**DOST-ITDI AI Training Workshop**  \n","**Day 1-2 Bridge Session: From Molecules to Images**\n","\n","---\n","\n","## Learning Objectives\n","1. Understand images as numerical data\n","2. Apply basic image processing techniques\n","3. Implement image augmentation strategies\n","4. Build and train Convolutional Neural Networks (CNNs)\n","5. Apply transfer learning to scientific images\n","6. Connect image analysis to molecular structure analysis\n","\n","## Why Computer Vision for Chemistry?\n","- Analyze microscopy images (crystals, cells, materials)\n","- Classify chemical structures from images\n","- Quality control in manufacturing\n","- Automated lab equipment reading\n","- Document digitization and OCR\n","- Molecular visualization and analysis\n","\n","**Key Insight**: Images are just another type of data, like molecular descriptors we've been using!"]},{"cell_type":"markdown","metadata":{"id":"YMknzsThRihZ"},"source":["## Section 1: Images as Data\n","\n","### 1.1 Setup and Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bB8nJguRihb"},"outputs":[],"source":["# Install required libraries\n","!pip install opencv-python pillow scikit-image torch torchvision rdkit -q\n","\n","print(\"[OK] Libraries installed successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndbYhpYsRihf"},"outputs":[],"source":["# Import libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from PIL import Image\n","import cv2\n","from skimage import filters, feature, exposure\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Deep Learning\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms, models\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Chemistry\n","from rdkit import Chem\n","from rdkit.Chem import Draw\n","\n","# Scikit-learn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","# Set seeds for reproducibility\n","np.random.seed(42)\n","torch.manual_seed(42)\n","\n","# Plotting style\n","sns.set_style(\"whitegrid\")\n","plt.rcParams['figure.figsize'] = (12, 6)\n","\n","print(\"[OK] All libraries imported!\")"]},{"cell_type":"markdown","metadata":{"id":"Zsgv-XKzRihi"},"source":["### 1.2 Load and Display Images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ml17_zMRihl"},"outputs":[],"source":["# Create a sample molecular structure image\n","# Using Aspirin - a molecule we're familiar with!\n","mol = Chem.MolFromSmiles('CC(=O)Oc1ccccc1C(=O)O')  # Aspirin\n","img = Draw.MolToImage(mol, size=(400, 400))\n","\n","# Display\n","plt.figure(figsize=(8, 8))\n","plt.imshow(img)\n","plt.title('Aspirin Molecular Structure', fontsize=14, fontweight='bold')\n","plt.axis('off')\n","plt.show()\n","\n","print(f\"Image type: {type(img)}\")\n","print(f\"Image size: {img.size}\")\n","print(f\"Image mode: {img.mode}\")"]},{"cell_type":"markdown","metadata":{"id":"OTaq4l31Rihn"},"source":["### 1.3 Images as Numpy Arrays"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hogmh8qeRihp"},"outputs":[],"source":["# Convert to numpy array\n","img_array = np.array(img)\n","\n","print(f\"Array shape: {img_array.shape}\")  # (height, width, channels)\n","print(f\"Data type: {img_array.dtype}\")\n","print(f\"Value range: [{img_array.min()}, {img_array.max()}]\")\n","\n","# Visualize RGB channels\n","fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n","\n","# Original\n","axes[0].imshow(img_array)\n","axes[0].set_title('Original Image')\n","axes[0].axis('off')\n","\n","# Red channel\n","axes[1].imshow(img_array[:,:,0], cmap='Reds')\n","axes[1].set_title('Red Channel')\n","axes[1].axis('off')\n","\n","# Green channel\n","axes[2].imshow(img_array[:,:,1], cmap='Greens')\n","axes[2].set_title('Green Channel')\n","axes[2].axis('off')\n","\n","# Blue channel\n","axes[3].imshow(img_array[:,:,2], cmap='Blues')\n","axes[3].set_title('Blue Channel')\n","axes[3].axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\n[KEY INSIGHT] Images are just 3D arrays of numbers!\")\n","print(\"Similar to how molecules are arrays of descriptors.\")"]},{"cell_type":"markdown","metadata":{"id":"AnOoSagWRihr"},"source":["### 1.4 Grayscale Conversion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfb0S7KeRiht"},"outputs":[],"source":["# Convert to grayscale\n","img_gray = np.array(img.convert('L'))\n","\n","print(f\"Grayscale shape: {img_gray.shape}\")  # (height, width) - no channels!\n","print(f\"Values range: [{img_gray.min()}, {img_gray.max()}]\")\n","\n","# Display comparison\n","fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n","\n","axes[0].imshow(img_array)\n","axes[0].set_title('RGB Image (3 channels)')\n","axes[0].axis('off')\n","\n","axes[1].imshow(img_gray, cmap='gray')\n","axes[1].set_title('Grayscale Image (1 channel)')\n","axes[1].axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"[INFO] Grayscale simplifies processing - one number per pixel instead of three!\")"]},{"cell_type":"markdown","metadata":{"id":"2n5Z0Zz2Rihv"},"source":["### 1.5 Generate Dataset of Molecular Images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2GEYgnhRihw"},"outputs":[],"source":["# Generate images for molecules from ESOL dataset\n","# These are the same molecules we used in notebooks 01, 02, and 04!\n","\n","url = \"https://raw.githubusercontent.com/deepchem/deepchem/master/datasets/delaney-processed.csv\"\n","df = pd.read_csv(url)\n","\n","print(f\"Loaded {len(df)} molecules from ESOL dataset\")\n","print(\"These are the same molecules we used in previous notebooks!\")\n","print(f\"\\nDataset columns: {df.columns.tolist()}\")\n","\n","# Generate images for first 100 molecules\n","n_samples = 100\n","molecular_images = []\n","molecular_labels = []\n","\n","for idx in range(n_samples):\n","    row = df.iloc[idx]\n","    mol = Chem.MolFromSmiles(row['smiles'])\n","\n","    if mol is not None:\n","        # Generate image\n","        img = Draw.MolToImage(mol, size=(200, 200))\n","        img_array = np.array(img.convert('L'))  # Grayscale\n","        molecular_images.append(img_array)\n","\n","        # Create label: molecules with rings vs without rings\n","        # Count aromatic rings using SMILES\n","        num_rings = row['smiles'].count('c') > 0  # Simple check for aromatic\n","        label = 1 if num_rings else 0\n","        molecular_labels.append(label)\n","\n","molecular_images = np.array(molecular_images)\n","molecular_labels = np.array(molecular_labels)\n","\n","print(f\"\\nGenerated {len(molecular_images)} molecular structure images\")\n","print(f\"Image array shape: {molecular_images.shape}\")\n","print(f\"Labels: {len(molecular_labels)}\")\n","print(f\"\\nLabel distribution:\")\n","print(f\"  Cyclic (with rings): {(molecular_labels == 1).sum()}\")\n","print(f\"  Acyclic (no rings): {(molecular_labels == 0).sum()}\")"]},{"cell_type":"markdown","metadata":{"id":"KwlzXcc_Rihy"},"source":["### 1.6 Visualize Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCxDfWPlRihz"},"outputs":[],"source":["# Display sample molecules\n","fig, axes = plt.subplots(3, 6, figsize=(15, 8))\n","axes = axes.ravel()\n","\n","for idx in range(18):\n","    axes[idx].imshow(molecular_images[idx], cmap='gray')\n","    label_text = 'Cyclic' if molecular_labels[idx] == 1 else 'Acyclic'\n","    axes[idx].set_title(f'{label_text}', fontsize=9)\n","    axes[idx].axis('off')\n","\n","plt.suptitle('Sample Molecular Structure Images', fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"[INFO] These are visual representations of molecules we've analyzed before!\")\n","print(\"We predicted their properties from SMILES and descriptors.\")\n","print(\"Now we'll use their visual structure directly.\")"]},{"cell_type":"markdown","metadata":{"id":"YJOre216Rih0"},"source":["## Section 2: Image Processing & Enhancement\n","\n","### 2.1 Edge Detection - Sobel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YCS5oBGRih1"},"outputs":[],"source":["# Edge detection finds boundaries (like molecular bonds!)\n","test_img = molecular_images[10]\n","\n","# Apply Sobel edge detection\n","edges_sobel = filters.sobel(test_img)\n","\n","# Display\n","fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","axes[0].imshow(test_img, cmap='gray')\n","axes[0].set_title('Original Molecular Structure')\n","axes[0].axis('off')\n","\n","axes[1].imshow(edges_sobel, cmap='gray')\n","axes[1].set_title('Sobel Edge Detection')\n","axes[1].axis('off')\n","\n","axes[2].imshow(test_img, cmap='gray', alpha=0.7)\n","axes[2].imshow(edges_sobel, cmap='Reds', alpha=0.5)\n","axes[2].set_title('Overlay: Edges Highlighted')\n","axes[2].axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"[INFO] Edge detection highlights molecular bonds and structure!\")\n","print(\"Similar to how we extracted structural features (rings, bonds) from SMILES.\")"]},{"cell_type":"markdown","metadata":{"id":"6MHtMO_uRih1"},"source":["### 2.2 Edge Detection - Canny"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBkgeBRfRih2"},"outputs":[],"source":["# Canny edge detection (more sophisticated)\n","edges_canny = feature.canny(test_img/255.0, sigma=2)\n","\n","fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n","\n","axes[0].imshow(edges_sobel, cmap='gray')\n","axes[0].set_title('Sobel Edge Detection')\n","axes[0].axis('off')\n","\n","axes[1].imshow(edges_canny, cmap='gray')\n","axes[1].set_title('Canny Edge Detection')\n","axes[1].axis('off')\n","\n","plt.suptitle('Comparing Edge Detection Methods', fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"[INFO] Canny produces cleaner edges - useful for identifying molecular features!\")"]},{"cell_type":"markdown","metadata":{"id":"lS9_2NnSRih4"},"source":["### 2.3 Image Enhancement - Contrast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-w4zS-HRih5"},"outputs":[],"source":["# Enhance contrast to make features more visible\n","\n","# Create low-contrast version\n","low_contrast = exposure.rescale_intensity(test_img, in_range=(50, 200))\n","\n","# Enhance with different methods\n","eq_hist = exposure.equalize_hist(low_contrast)\n","eq_adaptive = exposure.equalize_adapthist(low_contrast)\n","\n","# Display\n","fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n","\n","axes[0,0].imshow(test_img, cmap='gray')\n","axes[0,0].set_title('Original')\n","axes[0,0].axis('off')\n","\n","axes[0,1].imshow(low_contrast, cmap='gray')\n","axes[0,1].set_title('Low Contrast')\n","axes[0,1].axis('off')\n","\n","axes[1,0].imshow(eq_hist, cmap='gray')\n","axes[1,0].set_title('Histogram Equalization')\n","axes[1,0].axis('off')\n","\n","axes[1,1].imshow(eq_adaptive, cmap='gray')\n","axes[1,1].set_title('Adaptive Histogram Equalization')\n","axes[1,1].axis('off')\n","\n","plt.suptitle('Image Enhancement Techniques', fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"[INFO] Contrast enhancement makes molecular features more visible!\")\n","print(\"Like feature scaling in ML - brings out important patterns.\")"]},{"cell_type":"markdown","metadata":{"id":"hsUQZ9sERih6"},"source":["## Section 3: Image Augmentation\n","\n","### 3.1 Introduction to Image Augmentation\n","\n","Remember data augmentation from Classification (SMOTE, noise, mixup)?  \n","**Same principle for images!**\n","\n","#### Why Augment Images?\n","- Increase training data\n","- Improve model robustness\n","- Prevent overfitting\n","- Simulate real-world variations\n","\n","#### Common Transformations:\n","1. **Geometric:** rotation, flipping, scaling\n","2. **Color:** brightness, contrast, saturation\n","3. **Noise:** Gaussian, salt-and-pepper\n","4. **Cropping and padding**\n","\n","**Key Insight**: These simulate how the same molecule might appear in different conditions:\n","- Different orientations\n","- Different lighting\n","- Different image quality\n","- Different scales"]},{"cell_type":"markdown","metadata":{"id":"kK2p4I5lRih7"},"source":["### 3.2 Geometric Transformations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2gLEXPQRih7"},"outputs":[],"source":["# Define transformations\n","transform_rotate = transforms.RandomRotation(degrees=30)\n","transform_hflip = transforms.RandomHorizontalFlip(p=1.0)\n","transform_vflip = transforms.RandomVerticalFlip(p=1.0)\n","transform_affine = transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))\n","\n","# Convert to PIL Image\n","test_pil = Image.fromarray(test_img)\n","\n","# Apply transformations\n","rotated = transform_rotate(test_pil)\n","hflipped = transform_hflip(test_pil)\n","vflipped = transform_vflip(test_pil)\n","affine = transform_affine(test_pil)\n","\n","# Display\n","fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n","\n","axes[0,0].imshow(test_img, cmap='gray')\n","axes[0,0].set_title('Original')\n","axes[0,0].axis('off')\n","\n","axes[0,1].imshow(rotated, cmap='gray')\n","axes[0,1].set_title('Rotated (+/- 30 degrees)')\n","axes[0,1].axis('off')\n","\n","axes[0,2].imshow(hflipped, cmap='gray')\n","axes[0,2].set_title('Horizontal Flip')\n","axes[0,2].axis('off')\n","\n","axes[1,0].imshow(vflipped, cmap='gray')\n","axes[1,0].set_title('Vertical Flip')\n","axes[1,0].axis('off')\n","\n","axes[1,1].imshow(affine, cmap='gray')\n","axes[1,1].set_title('Translation (shifted)')\n","axes[1,1].axis('off')\n","\n","axes[1,2].axis('off')\n","\n","plt.suptitle('Geometric Augmentations', fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"[INFO] Geometric transformations create different views of the same molecule!\")\n","print(\"The molecular structure is the same, just oriented differently.\")"]},{"cell_type":"markdown","metadata":{"id":"3VeBpHCfRih8"},"source":["### 3.3 Complete Augmentation Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umKUZhx5Rih8"},"outputs":[],"source":["# Create comprehensive augmentation pipeline\n","augmentation_pipeline = transforms.Compose([\n","    transforms.RandomRotation(degrees=15),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n","])\n","\n","# Generate augmented versions\n","n_augmentations = 8\n","augmented_samples = []\n","\n","for i in range(n_augmentations):\n","    aug_img = augmentation_pipeline(test_pil)\n","    augmented_samples.append(aug_img)\n","\n","# Display original + augmentations\n","fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n","axes = axes.ravel()\n","\n","axes[0].imshow(test_pil, cmap='gray')\n","axes[0].set_title('Original', fontsize=11, fontweight='bold')\n","axes[0].axis('off')\n","\n","for idx in range(n_augmentations):\n","    axes[idx+1].imshow(augmented_samples[idx], cmap='gray')\n","    axes[idx+1].set_title(f'Augmented {idx+1}', fontsize=10)\n","    axes[idx+1].axis('off')\n","\n","plt.suptitle('Augmentation Pipeline: 1 Image -> 8 Variations',\n","             fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"From 1 original image, created {n_augmentations} augmented versions!\")\n","print(f\"Dataset size increased by {n_augmentations+1}x\")\n","print(\"\\nEach augmentation maintains the molecular structure\")\n","print(\"but presents it from different perspectives.\")\n","print(\"\\n[CONNECTION] Remember molecular augmentation (SMOTE, noise)?\")\n","print(\"Same principle - expand dataset to improve model performance!\")"]},{"cell_type":"markdown","metadata":{"id":"IPy6LJH0Rih-"},"source":["## Section 4: Convolutional Neural Networks (CNNs)\n","\n","### 4.1 What are CNNs?\n","\n","Neural networks designed specifically for images.\n","\n","#### Key Difference from Regular Neural Networks:\n","- **Regular NN:** Treats image as flat array (loses spatial structure)\n","- **CNN:** Preserves spatial relationships between pixels\n","\n","#### Layers in a CNN:\n","1. **Convolutional Layer:** Learns local patterns (edges, textures)\n","2. **Pooling Layer:** Reduces size, keeps important features\n","3. **Fully Connected Layer:** Makes final classification\n","\n","#### Why CNNs for Images?\n","- Automatically learn features (edges, shapes, objects)\n","- Parameter efficient (shared weights)\n","- Translation invariant (finds patterns anywhere in image)\n","\n","**Connection**: Like how neural networks learned molecular patterns automatically,  \n","CNNs learn visual patterns automatically!"]},{"cell_type":"markdown","metadata":{"id":"SX08nDPXRih-"},"source":["### 4.2 Simple CNN Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMckW7OrRih_"},"outputs":[],"source":["# Define a simple CNN for molecular image classification\n","class SimpleMolecularCNN(nn.Module):\n","    def __init__(self, num_classes=2):\n","        super(SimpleMolecularCNN, self).__init__()\n","\n","        # Convolutional layers\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # 1 channel (grayscale) -> 16\n","        self.relu1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d(2, 2)  # Reduce size by half\n","\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # 16 -> 32 channels\n","        self.relu2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 32 -> 64 channels\n","        self.relu3 = nn.ReLU()\n","        self.pool3 = nn.MaxPool2d(2, 2)\n","\n","        # Fully connected layers\n","        # After 3 pooling layers: 200/2/2/2 = 25\n","        self.fc1 = nn.Linear(64 * 25 * 25, 128)\n","        self.relu4 = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        # Convolutional layers\n","        x = self.pool1(self.relu1(self.conv1(x)))\n","        x = self.pool2(self.relu2(self.conv2(x)))\n","        x = self.pool3(self.relu3(self.conv3(x)))\n","\n","        # Flatten\n","        x = x.view(x.size(0), -1)\n","\n","        # Fully connected layers\n","        x = self.relu4(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        return x\n","\n","# Create model\n","model = SimpleMolecularCNN(num_classes=2)\n","print(model)\n","print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","\n","# Test forward pass\n","test_input = torch.randn(1, 1, 200, 200)  # Batch=1, Channels=1, H=200, W=200\n","output = model(test_input)\n","print(f\"\\nInput shape: {test_input.shape}\")\n","print(f\"Output shape: {output.shape}\")\n","print(f\"Output values: {output}\")"]},{"cell_type":"markdown","metadata":{"id":"o9pP9XZfRih_"},"source":["### 4.3 Prepare Data for Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jg-kM_LkRih_"},"outputs":[],"source":["# Custom Dataset class\n","class MolecularImageDataset(Dataset):\n","    \"\"\"Custom dataset for molecular images\"\"\"\n","    def __init__(self, images, labels, transform=None):\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        label = self.labels[idx]\n","\n","        # Convert to PIL Image\n","        image = Image.fromarray(image)\n","\n","        # Apply transformations\n","        if self.transform:\n","            image = self.transform(image)\n","        else:\n","            # Default: convert to tensor and normalize\n","            image = transforms.ToTensor()(image)\n","\n","        return image, label\n","\n","# Define transformations\n","train_transform = transforms.Compose([\n","    transforms.RandomRotation(15),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(\n","    molecular_images, molecular_labels, test_size=0.2, random_state=42, stratify=molecular_labels\n",")\n","\n","print(f\"Training set: {len(X_train)} images\")\n","print(f\"Test set: {len(X_test)} images\")\n","print(f\"\\nTraining class distribution:\")\n","print(f\"  Cyclic: {(y_train == 1).sum()}\")\n","print(f\"  Acyclic: {(y_train == 0).sum()}\")\n","\n","# Create datasets\n","train_dataset = MolecularImageDataset(X_train, y_train, transform=train_transform)\n","test_dataset = MolecularImageDataset(X_test, y_test, transform=test_transform)\n","\n","# Create dataloaders\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","print(f\"\\nDataLoaders created:\")\n","print(f\"  Training batches: {len(train_loader)}\")\n","print(f\"  Test batches: {len(test_loader)}\")\n","print(f\"  Batch size: {batch_size}\")"]},{"cell_type":"markdown","metadata":{"id":"0pFD8LevRih_"},"source":["### 4.4 Train the CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zsgge-YgRiiA"},"outputs":[],"source":["# Training setup\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","model = SimpleMolecularCNN(num_classes=2).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training function\n","def train_epoch(model, train_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Statistics\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    epoch_loss = running_loss / len(train_loader)\n","    epoch_acc = 100 * correct / total\n","    return epoch_loss, epoch_acc\n","\n","# Validation function\n","def validate(model, test_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    epoch_loss = running_loss / len(test_loader)\n","    epoch_acc = 100 * correct / total\n","    return epoch_loss, epoch_acc\n","\n","# Train model\n","num_epochs = 20\n","train_losses = []\n","train_accs = []\n","val_losses = []\n","val_accs = []\n","\n","print(\"Training CNN...\")\n","print(\"=\"*60)\n","\n","for epoch in range(num_epochs):\n","    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n","    val_loss, val_acc = validate(model, test_loader, criterion, device)\n","\n","    train_losses.append(train_loss)\n","    train_accs.append(train_acc)\n","    val_losses.append(val_loss)\n","    val_accs.append(val_acc)\n","\n","    if (epoch + 1) % 5 == 0:\n","        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n","        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n","        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n","\n","print(\"\\n[OK] Training complete!\")\n","print(f\"Final validation accuracy: {val_accs[-1]:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"0rLI-S_4RiiB"},"source":["### 4.5 Visualize Training Progress"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQyI6YivRiiB"},"outputs":[],"source":["# Plot training curves\n","fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","\n","# Loss\n","axes[0].plot(train_losses, label='Training Loss', linewidth=2)\n","axes[0].plot(val_losses, label='Validation Loss', linewidth=2)\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Loss')\n","axes[0].set_title('Training and Validation Loss')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","# Accuracy\n","axes[1].plot(train_accs, label='Training Accuracy', linewidth=2)\n","axes[1].plot(val_accs, label='Validation Accuracy', linewidth=2)\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Accuracy (%)')\n","axes[1].set_title('Training and Validation Accuracy')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Check for overfitting\n","final_train_acc = train_accs[-1]\n","final_val_acc = val_accs[-1]\n","gap = abs(final_train_acc - final_val_acc)\n","\n","print(f\"\\nFinal Training Accuracy: {final_train_acc:.2f}%\")\n","print(f\"Final Validation Accuracy: {final_val_acc:.2f}%\")\n","print(f\"Gap: {gap:.2f}%\")\n","\n","if gap > 15:\n","    print(\"\\n[!] Large gap suggests overfitting. Try:\")\n","    print(\"  - More data augmentation\")\n","    print(\"  - Higher dropout rate\")\n","    print(\"  - Simpler architecture\")\n","else:\n","    print(\"\\n[OK] Model appears to be generalizing well!\")"]},{"cell_type":"markdown","metadata":{"id":"8URKxmcdRiiD"},"source":["### 4.6 Evaluate Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvQeLZV0RiiF"},"outputs":[],"source":["# Get predictions on test set\n","model.eval()\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        all_preds.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.numpy())\n","\n","all_preds = np.array(all_preds)\n","all_labels = np.array(all_labels)\n","\n","# Confusion matrix\n","cm = confusion_matrix(all_labels, all_preds)\n","\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n","            xticklabels=['Acyclic', 'Cyclic'],\n","            yticklabels=['Acyclic', 'Cyclic'])\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Confusion Matrix: Molecular Structure Classification')\n","plt.tight_layout()\n","plt.show()\n","\n","# Detailed metrics\n","print(\"\\nClassification Report:\")\n","print(\"=\"*60)\n","print(classification_report(all_labels, all_preds,\n","                           target_names=['Acyclic', 'Cyclic']))"]},{"cell_type":"markdown","source":["### 5.4 Summary: Understanding How CNNs Learn\n","\n","**What We Discovered:**\n","\n","1. **Learned Filters (5.1)**\n","   - Each filter is a small pattern detector (3×3 pixels)\n","   - First layer learns basic features: edges, lines, corners\n","   - These are automatically learned, not hand-coded!\n","\n","2. **Feature Maps (5.2)**\n","   - Shows how each filter responds to the input image\n","   - Earlier layers → Simple features (edges)\n","   - Deeper layers → Complex features (shapes, structures)\n","   - Progressive abstraction from pixels to concepts\n","\n","3. **Filter Preferences (5.3)**\n","   - Different filters activate for different patterns\n","   - Filters specialize in detecting specific molecular features\n","   - This is how the network distinguishes cyclic from acyclic molecules\n","\n","**Key Insights:**\n","\n","- **Hierarchical Learning**:\n","  - Layer 1: Edges and textures\n","  - Layer 2: Shapes and parts\n","  - Layer 3: Complex molecular structures\n","  \n","- **Automatic Feature Engineering**:\n","  - No manual feature extraction needed\n","  - Network learns what's important for the task\n","  - Compare with: Manual RDKit descriptors (LogP, MW, etc.)\n","\n","- **Interpretability**:\n","  - We can peek inside the \"black box\"\n","  - Understand what the network learned\n","  - Debug and improve architecture\n","\n","**Connection to Other Concepts:**\n","\n","| Concept | Manual Approach | CNN Approach |\n","|---------|----------------|--------------|\n","| **Feature Extraction** | RDKit descriptors | Learned filters |\n","| **Feature Selection** | Correlation analysis | Network learns importance |\n","| **Hierarchy** | Expert knowledge | Automatic layer-by-layer |\n","| **Interpretability** | Clear feature names | Visualize activations |\n","\n","**Why This Matters:**\n","\n","Understanding how CNNs learn helps us:\n","- Design better architectures\n","- Diagnose model failures\n","- Build trust in predictions\n","- Combine with domain knowledge\n","\n","**Next Steps:**\n","\n","If you want to go deeper:\n","- Try visualizing filters from conv2 and conv3\n","- Use Grad-CAM to see which image regions matter most\n","- Apply transfer learning with pretrained networks\n","- Combine CNN features with RDKit descriptors"],"metadata":{"id":"E9cT90osRiiG"}},{"cell_type":"markdown","metadata":{"id":"Yc7zJeKdRiiH"},"source":["## Summary\n","\n","### What We Learned:\n","\n","1. **Images as Data**\n","   - Images are numerical arrays (height × width × channels)\n","   - Can be processed like any other ML data\n","   - Generated molecular structure images from familiar ESOL dataset\n","\n","2. **Image Processing**\n","   - Edge detection: Find molecular structures\n","   - Enhancement: Improve image quality\n","   - Preprocessing improves ML performance\n","\n","3. **Image Augmentation**\n","   - Geometric: rotation, flipping, translation\n","   - **Connection**: Same principle as molecular augmentation!\n","   - Expands dataset, improves robustness\n","\n","4. **Convolutional Neural Networks**\n","   - Automatic feature learning\n","   - Preserve spatial structure\n","   - Hierarchical pattern recognition\n","   - Trained CNN to classify molecular structures\n","\n","### Key Takeaways:\n","\n","1. **Multi-Modal Learning**: Same molecules, different representations\n","   - SMILES notation → Molecular descriptors → Images\n","   - Each modality captures different aspects\n","\n","2. **Augmentation Everywhere**:\n","   - Molecular features: noise, SMOTE, mixup\n","   - Images: rotation, brightness, noise\n","   - **Universal principle**: Expand small datasets\n","\n","3. **Automatic vs Manual Features**:\n","   - Manual: RDKit descriptors, image filters\n","   - Automatic: Neural networks, CNNs\n","   - Trade-off: interpretability vs performance\n","\n","### Connections to Other Notebooks:\n","\n","| Notebook | Connection |\n","|----------|------------|\n","| **01_EDA** | Same ESOL molecules, visualized as images |\n","| **02_Regression** | Predicted properties from descriptors |\n","| **03_Classification** | Same augmentation principles (SMOTE, noise) |\n","| **04_PyTorch** | Same training loops, Dataset classes |\n","| **04b_HuggingFace** | Transfer learning concept (ChemBERTa) |\n","| **05_LLMs** | Image generation completes the workflow! |\n","\n","### Resources:\n","\n","- [OpenCV](https://opencv.org/)\n","- [scikit-image](https://scikit-image.org/)\n","- [PyTorch Vision](https://pytorch.org/vision/)\n","- [CS231n](http://cs231n.stanford.edu/) - Stanford CV course\n","\n","---\n","\n","**Great job! You now understand computer vision fundamentals for scientific research!**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[{"file_id":"https://github.com/jomaminoza/dost-ai-training/blob/main/notebooks/06_Computer_Vision.ipynb","timestamp":1764397307534}]}},"nbformat":4,"nbformat_minor":0}