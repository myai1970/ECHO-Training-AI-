{"cells":[{"cell_type":"markdown","metadata":{"id":"VgIRrLuwQNmw"},"source":["# Deep Learning with PyTorch\n","\n","**DOST-ITDI AI Training Workshop**  \n","**Day 1 - Session 4: Deep Learning Fundamentals with PyTorch**\n","\n","---\n","\n","## Learning Objectives\n","1. Understand neural networks and deep learning\n","2. Build neural networks with PyTorch\n","3. Train models with custom training loops\n","4. Apply deep learning to molecular property prediction\n","5. Understand overfitting and regularization\n","\n","## What is Deep Learning?\n","\n","Deep Learning uses neural networks with multiple layers to learn complex patterns from data.\n","\n","**Advantages**:\n","- Can learn complex, non-linear relationships\n","- Automatic feature learning\n","- State-of-the-art performance on many tasks\n","\n","**Chemistry Applications**:\n","- Molecular property prediction\n","- Drug-target interaction\n","- Reaction prediction\n","- Molecular generation"]},{"cell_type":"markdown","metadata":{"id":"WYmA7jvyQNmz"},"source":["## 1. Setup and Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ln3_mGKQNm0"},"outputs":[],"source":["# Install PyTorch and dependencies\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q\n","!pip install rdkit scikit-learn -q\n","\n","print(\"✓ Installation complete!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sy4eBFiuQNm3"},"outputs":[],"source":["# Import libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from rdkit import Chem\n","from rdkit.Chem import Descriptors\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","\n","# Scikit-learn\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n","\n","# Set random seeds for reproducibility\n","np.random.seed(42)\n","torch.manual_seed(42)\n","\n","# Check if CUDA is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","print(f\"PyTorch version: {torch.__version__}\")\n","\n","# Plotting\n","sns.set_style(\"whitegrid\")\n","plt.rcParams['figure.figsize'] = (10, 6)"]},{"cell_type":"markdown","metadata":{"id":"ck0FU2yaQNm5"},"source":["## 2. PyTorch Basics\n","\n","### 2.1 Tensors - The Building Blocks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"coBKtBpbQNm6"},"outputs":[],"source":["# Creating tensors\n","# Similar to NumPy arrays but can run on GPU\n","\n","# From Python lists\n","x = torch.tensor([1, 2, 3, 4, 5])\n","print(\"1D Tensor:\", x)\n","print(\"Shape:\", x.shape)\n","print(\"Data type:\", x.dtype)\n","\n","# 2D tensor (matrix)\n","y = torch.tensor([[1, 2, 3],\n","                  [4, 5, 6]])\n","print(\"\\n2D Tensor:\\n\", y)\n","print(\"Shape:\", y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ckp4a9MSQNm7"},"outputs":[],"source":["# Creating tensors with specific values\n","zeros = torch.zeros(3, 4)\n","ones = torch.ones(2, 3)\n","random = torch.randn(2, 3)  # Random normal distribution\n","\n","print(\"Zeros:\\n\", zeros)\n","print(\"\\nOnes:\\n\", ones)\n","print(\"\\nRandom:\\n\", random)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6cKJqzpQNm8"},"outputs":[],"source":["# Tensor operations\n","a = torch.tensor([1.0, 2.0, 3.0])\n","b = torch.tensor([4.0, 5.0, 6.0])\n","\n","print(\"Addition:\", a + b)\n","print(\"Multiplication:\", a * b)\n","print(\"Dot product:\", torch.dot(a, b))\n","print(\"Mean:\", a.mean())\n","print(\"Sum:\", a.sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fpfO_3WOQNm9"},"outputs":[],"source":["# Converting between NumPy and PyTorch\n","numpy_array = np.array([1, 2, 3, 4, 5])\n","tensor_from_numpy = torch.from_numpy(numpy_array)\n","print(\"From NumPy:\", tensor_from_numpy)\n","\n","tensor = torch.tensor([6, 7, 8, 9, 10])\n","numpy_from_tensor = tensor.numpy()\n","print(\"To NumPy:\", numpy_from_tensor)"]},{"cell_type":"markdown","metadata":{"id":"8M1ZOwgBQNm_"},"source":["### 2.2 Autograd - Automatic Differentiation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HA0JxOnxQNnA"},"outputs":[],"source":["# PyTorch can automatically compute gradients\n","# This is essential for training neural networks\n","\n","x = torch.tensor([2.0], requires_grad=True)  # Track gradients for this tensor\n","y = x ** 2 + 3 * x + 1  # y = x² + 3x + 1\n","\n","print(f\"x = {x.item()}\")\n","print(f\"y = {y.item()}\")\n","\n","# Compute gradient dy/dx\n","y.backward()  # Calculate gradients\n","print(f\"dy/dx = {x.grad.item()}\")  # Should be 2x + 3 = 7 when x=2"]},{"cell_type":"markdown","metadata":{"id":"xAMF0D2FQNnB"},"source":["## 3. Building Neural Networks\n","\n","### 3.1 Simple Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgReB8jDQNnC"},"outputs":[],"source":["# Define a simple neural network\n","class SimpleNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(SimpleNN, self).__init__()\n","        # Define layers\n","        self.fc1 = nn.Linear(input_size, hidden_size)  # First layer\n","        self.relu = nn.ReLU()  # Activation function\n","        self.fc2 = nn.Linear(hidden_size, output_size)  # Output layer\n","\n","    def forward(self, x):\n","        # Define forward pass\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x\n","\n","# Create model\n","model = SimpleNN(input_size=10, hidden_size=20, output_size=1)\n","print(model)\n","\n","# Count parameters\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"\\nTotal parameters: {total_params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIiCuWHNQNnD"},"outputs":[],"source":["# Test forward pass\n","sample_input = torch.randn(1, 10)  # Batch size=1, features=10\n","output = model(sample_input)\n","print(f\"Input shape: {sample_input.shape}\")\n","print(f\"Output shape: {output.shape}\")\n","print(f\"Output value: {output.item():.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"p_wtYngvQNnD"},"source":["## 4. Loading and Preparing Chemistry Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMKELtesQNnD"},"outputs":[],"source":["# Load ESOL solubility dataset\n","url = \"https://raw.githubusercontent.com/deepchem/deepchem/master/datasets/delaney-processed.csv\"\n","df = pd.read_csv(url)\n","\n","print(f\"Dataset shape: {df.shape}\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NG-f_RxMQNnE"},"outputs":[],"source":["# Feature engineering\n","df['mol'] = df['smiles'].apply(Chem.MolFromSmiles)\n","df = df[df['mol'].notna()].copy()\n","\n","df['LogP'] = df['mol'].apply(Descriptors.MolLogP)\n","df['NumHAcceptors'] = df['mol'].apply(Descriptors.NumHAcceptors)\n","df['NumAromaticRings'] = df['mol'].apply(Descriptors.NumAromaticRings)\n","\n","# Select features\n","feature_columns = [\n","    'Molecular Weight',\n","    'Number of H-Bond Donors',\n","    'Number of Rings',\n","    'Number of Rotatable Bonds',\n","    'Polar Surface Area',\n","    'LogP',\n","    'NumHAcceptors',\n","    'NumAromaticRings'\n","]\n","\n","X = df[feature_columns].values\n","y = df['measured log solubility in mols per litre'].values.reshape(-1, 1)\n","\n","print(f\"Features shape: {X.shape}\")\n","print(f\"Target shape: {y.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ho-5GsBQNnF"},"outputs":[],"source":["# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Feature scaling\n","scaler_X = StandardScaler()\n","scaler_y = StandardScaler()\n","\n","X_train_scaled = scaler_X.fit_transform(X_train)\n","X_test_scaled = scaler_X.transform(X_test)\n","y_train_scaled = scaler_y.fit_transform(y_train)\n","y_test_scaled = scaler_y.transform(y_test)\n","\n","print(f\"Training set: {X_train_scaled.shape[0]} samples\")\n","print(f\"Test set: {X_test_scaled.shape[0]} samples\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkNinYZxQNnF"},"outputs":[],"source":["# Convert to PyTorch tensors\n","X_train_tensor = torch.FloatTensor(X_train_scaled)\n","y_train_tensor = torch.FloatTensor(y_train_scaled)\n","X_test_tensor = torch.FloatTensor(X_test_scaled)\n","y_test_tensor = torch.FloatTensor(y_test_scaled)\n","\n","print(\"Tensor shapes:\")\n","print(f\"X_train: {X_train_tensor.shape}\")\n","print(f\"y_train: {y_train_tensor.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IccgfNJzQNnG"},"outputs":[],"source":["# Create DataLoader for batch training\n","batch_size = 32\n","\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","print(f\"Number of batches in train_loader: {len(train_loader)}\")\n","print(f\"Batch size: {batch_size}\")"]},{"cell_type":"markdown","metadata":{"id":"k1EAsg65QNnG"},"source":["## 5. Building a Molecular Property Predictor\n","\n","### 5.1 Define the Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gm9jBWDQNnG"},"outputs":[],"source":["class MolecularPredictor(nn.Module):\n","    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.2):\n","        super(MolecularPredictor, self).__init__()\n","\n","        # Input layer\n","        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n","        self.bn1 = nn.BatchNorm1d(hidden_sizes[0])  # Batch normalization\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(dropout)\n","\n","        # Hidden layer\n","        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n","        self.bn2 = nn.BatchNorm1d(hidden_sizes[1])\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","        # Output layer\n","        self.fc3 = nn.Linear(hidden_sizes[1], output_size)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.bn1(x)\n","        x = self.relu1(x)\n","        x = self.dropout1(x)\n","\n","        x = self.fc2(x)\n","        x = self.bn2(x)\n","        x = self.relu2(x)\n","        x = self.dropout2(x)\n","\n","        x = self.fc3(x)\n","        return x\n","\n","# Create model\n","input_size = X_train_scaled.shape[1]\n","hidden_sizes = [64, 32]\n","output_size = 1\n","\n","model = MolecularPredictor(input_size, hidden_sizes, output_size, dropout=0.2)\n","model = model.to(device)\n","\n","print(model)\n","print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"]},{"cell_type":"markdown","metadata":{"id":"dSOr65PIQNnH"},"source":["### 5.2 Define Loss Function and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"feODmLwrQNnH"},"outputs":[],"source":["# Loss function (Mean Squared Error for regression)\n","criterion = nn.MSELoss()\n","\n","# Optimizer (Adam)\n","learning_rate = 0.001\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Learning rate scheduler\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, mode='min', patience=10, factor=0.5\n",")\n","\n","print(f\"Optimizer: {optimizer}\")\n","print(f\"Loss function: {criterion}\")"]},{"cell_type":"markdown","metadata":{"id":"rjxOwcNrQNnI"},"source":["### 5.3 Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tu5yrYfpQNnI"},"outputs":[],"source":["# Training function\n","def train_epoch(model, train_loader, criterion, optimizer, device):\n","    model.train()  # Set to training mode\n","    running_loss = 0.0\n","\n","    for batch_X, batch_y in train_loader:\n","        batch_X = batch_X.to(device)\n","        batch_y = batch_y.to(device)\n","\n","        # Forward pass\n","        outputs = model(batch_X)\n","        loss = criterion(outputs, batch_y)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()  # Clear gradients\n","        loss.backward()  # Compute gradients\n","        optimizer.step()  # Update weights\n","\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(train_loader)\n","    return avg_loss\n","\n","# Validation function\n","def validate(model, test_loader, criterion, device):\n","    model.eval()  # Set to evaluation mode\n","    running_loss = 0.0\n","\n","    with torch.no_grad():  # Disable gradient computation\n","        for batch_X, batch_y in test_loader:\n","            batch_X = batch_X.to(device)\n","            batch_y = batch_y.to(device)\n","\n","            outputs = model(batch_X)\n","            loss = criterion(outputs, batch_y)\n","            running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(test_loader)\n","    return avg_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ac5RbZVGQNnJ"},"outputs":[],"source":["# Train the model\n","num_epochs = 100\n","train_losses = []\n","val_losses = []\n","best_val_loss = float('inf')\n","\n","print(\"Training started...\")\n","print(\"=\" * 60)\n","\n","for epoch in range(num_epochs):\n","    # Train\n","    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n","    train_losses.append(train_loss)\n","\n","    # Validate\n","    val_loss = validate(model, test_loader, criterion, device)\n","    val_losses.append(val_loss)\n","\n","    # Learning rate scheduler step\n","    scheduler.step(val_loss)\n","\n","    # Save best model\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","    # Print progress\n","    if (epoch + 1) % 10 == 0:\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n","              f\"Train Loss: {train_loss:.4f} | \"\n","              f\"Val Loss: {val_loss:.4f}\")\n","\n","print(\"\\n✓ Training complete!\")\n","print(f\"Best validation loss: {best_val_loss:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"r36UOJpeQNnJ"},"source":["### 5.4 Visualize Training Progress"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WulWVwRQNnK"},"outputs":[],"source":["# Plot training curves\n","plt.figure(figsize=(10, 6))\n","plt.plot(train_losses, label='Training Loss', linewidth=2)\n","plt.plot(val_losses, label='Validation Loss', linewidth=2)\n","plt.xlabel('Epoch', fontsize=12)\n","plt.ylabel('Loss (MSE)', fontsize=12)\n","plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')\n","plt.legend(fontsize=11)\n","plt.grid(True, alpha=0.3)\n","plt.show()\n","\n","# Check for overfitting\n","final_train_loss = train_losses[-1]\n","final_val_loss = val_losses[-1]\n","gap = abs(final_val_loss - final_train_loss)\n","\n","print(f\"\\nFinal Training Loss: {final_train_loss:.4f}\")\n","print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n","print(f\"Gap: {gap:.4f}\")\n","\n","if gap > 0.1:\n","    print(\"⚠️ Model may be overfitting (large gap between train and val loss)\")\n","else:\n","    print(\"✓ Model appears to be generalizing well\")"]},{"cell_type":"markdown","source":["#### Comparing Augmentation Techniques\n","\n","| Technique | How It Works | Pros | Cons | When to Use |\n","|-----------|--------------|------|------|-------------|\n","| **Gaussian Noise** | Add random noise to features | Simple, fast | May add unrealistic values | Small datasets, robust models needed |\n","| **Mixup** | Interpolate between samples | Smooth decision boundaries | Requires modified loss | Medium datasets, classification |\n","| **Feature Dropout** | Random zero features | Acts as regularization | Loss of information | High-dimensional data |\n","\n","**Best Practices:**\n","1. Start without augmentation - establish baseline\n","2. Try Gaussian noise first (simplest)\n","3. Use small noise std (0.01 - 0.1 for scaled data)\n","4. For Mixup, use alpha = 0.1-0.4\n","5. Monitor if augmentation helps or hurts\n","6. More useful for small datasets (<1000 samples)\n","\n","**For Molecular Data Specifically:**\n","- Noise augmentation simulates measurement uncertainty\n","- Mix up creates molecules with intermediate properties\n","- Be careful not to create chemically impossible values\n","- Consider domain constraints (e.g., MW > 0)"],"metadata":{"id":"ObXm5ekwQNnL"}},{"cell_type":"code","source":["# Mixup augmentation function\n","def mixup_data(x, y, alpha=0.2):\n","    \"\"\"\n","    Returns mixed inputs, pairs of targets, and lambda\n","    \"\"\"\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","# Example: Create mixup samples\n","sample1_idx = 0\n","sample2_idx = 10\n","\n","x1 = torch.FloatTensor(X_train_scaled[sample1_idx])\n","x2 = torch.FloatTensor(X_train_scaled[sample2_idx])\n","y1 = y_train_scaled[sample1_idx]\n","y2 = y_train_scaled[sample2_idx]\n","\n","# Create mixup samples with different lambdas\n","lambdas = [0.0, 0.25, 0.5, 0.75, 1.0]\n","mixup_samples = []\n","\n","for lam in lambdas:\n","    mixed_x = lam * x1 + (1 - lam) * x2\n","    mixed_y = lam * y1 + (1 - lam) * y2\n","    mixup_samples.append((mixed_x.numpy(), mixed_y))\n","\n","# Visualize mixup\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Feature values across mixup ratios\n","feature_idx = 0\n","lambda_values = []\n","feature_values = []\n","\n","for i, lam in enumerate(lambdas):\n","    lambda_values.append(lam)\n","    feature_values.append(mixup_samples[i][0][feature_idx])\n","\n","axes[0].plot(lambda_values, feature_values, 'o-', linewidth=2, markersize=10)\n","axes[0].axhline(y=x1[feature_idx].numpy(), color='r', linestyle='--',\n","               label=f'Sample 1: {x1[feature_idx]:.3f}')\n","axes[0].axhline(y=x2[feature_idx].numpy(), color='b', linestyle='--',\n","               label=f'Sample 2: {x2[feature_idx]:.3f}')\n","axes[0].set_xlabel('Lambda (λ)', fontsize=12)\n","axes[0].set_ylabel(f'Mixed Feature Value', fontsize=12)\n","axes[0].set_title(f'Mixup Interpolation ({feature_columns[0]})',\n","                 fontsize=13, fontweight='bold')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","# Target values across mixup ratios\n","target_values = [mixup_samples[i][1][0] for i in range(len(lambdas))]\n","\n","axes[1].plot(lambda_values, target_values, 's-', linewidth=2,\n","            markersize=10, color='green')\n","axes[1].axhline(y=y1[0], color='r', linestyle='--',\n","               label=f'Target 1: {y1[0]:.3f}')\n","axes[1].axhline(y=y2[0], color='b', linestyle='--',\n","               label=f'Target 2: {y2[0]:.3f}')\n","axes[1].set_xlabel('Lambda (λ)', fontsize=12)\n","axes[1].set_ylabel('Mixed Target Value', fontsize=12)\n","axes[1].set_title('Target Interpolation', fontsize=13, fontweight='bold')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"Mixup creates virtual samples between existing ones\")\n","print(f\"λ=0: 100% sample 2, λ=1: 100% sample 1\")\n","print(f\"λ=0.5: equal mix of both samples\")"],"metadata":{"id":"GaYHoIHsQNnL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Technique 2: Mixup\n","\n","Create virtual training samples by linearly interpolating between pairs of samples.\n","\n","**Formula:** x_new = λ × x1 + (1-λ) × x2, where λ ~ Beta(α, α)"],"metadata":{"id":"0zwJ79CqQNnM"}},{"cell_type":"code","source":["# Custom Dataset with Gaussian Noise Augmentation\n","class AugmentedDataset(Dataset):\n","    def __init__(self, X, y, noise_std=0.1, augment=True):\n","        self.X = torch.FloatTensor(X)\n","        self.y = torch.FloatTensor(y)\n","        self.noise_std = noise_std\n","        self.augment = augment\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        x = self.X[idx]\n","        y = self.y[idx]\n","\n","        # Add Gaussian noise during training\n","        if self.augment:\n","            noise = torch.randn_like(x) * self.noise_std\n","            x = x + noise\n","\n","        return x, y\n","\n","# Create augmented dataset\n","train_dataset_aug = AugmentedDataset(X_train_scaled, y_train_scaled,\n","                                      noise_std=0.05, augment=True)\n","train_loader_aug = DataLoader(train_dataset_aug, batch_size=32, shuffle=True)\n","\n","# Visualize effect of noise\n","sample_idx = 0\n","original = torch.FloatTensor(X_train_scaled[sample_idx])\n","augmented_samples = []\n","\n","for i in range(5):\n","    noise = torch.randn_like(original) * 0.05\n","    augmented = original + noise\n","    augmented_samples.append(augmented.numpy())\n","\n","# Plot\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Feature values comparison\n","feature_idx = 0  # First feature\n","axes[0].plot([0], original[feature_idx].numpy(), 'ro', markersize=15,\n","            label='Original', zorder=5)\n","for i, aug in enumerate(augmented_samples):\n","    axes[0].plot([i+1], aug[feature_idx], 'bo', alpha=0.6, markersize=10)\n","axes[0].set_xlabel('Sample Number', fontsize=12)\n","axes[0].set_ylabel(f'Feature Value ({feature_columns[0]})', fontsize=12)\n","axes[0].set_title('Effect of Gaussian Noise on One Feature', fontsize=13, fontweight='bold')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","# Distribution of augmented values\n","all_aug_values = [aug[feature_idx] for aug in augmented_samples]\n","axes[1].hist([original[feature_idx].numpy()], bins=20, alpha=0.7,\n","            label='Original', color='red', edgecolor='black')\n","axes[1].hist(all_aug_values, bins=20, alpha=0.7,\n","            label='Augmented', color='blue', edgecolor='black')\n","axes[1].set_xlabel(f'Feature Value', fontsize=12)\n","axes[1].set_ylabel('Count', fontsize=12)\n","axes[1].set_title('Distribution: Original vs Augmented', fontsize=13, fontweight='bold')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3, axis='y')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"Noise standard deviation: 0.05\")\n","print(f\"Original value: {original[feature_idx]:.4f}\")\n","print(f\"Augmented values: {[f'{v:.4f}' for v in all_aug_values]}\")"],"metadata":{"id":"k8W-KngBQNnN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Technique 1: Adding Gaussian Noise\n","\n","Add small random noise to feature values during training."],"metadata":{"id":"3JC_lOmdQNnO"}},{"cell_type":"markdown","source":["### 5.5 Data Augmentation for Molecular Data\n","\n","Data augmentation artificially increases training data by creating modified versions of existing samples.\n","\n","**For Images:** Rotation, flipping, cropping  \n","**For Molecular/Tabular Data:** Different approaches needed\n","\n","**Common Techniques:**\n","1. **Adding Noise:** Gaussian noise to feature values\n","2. **Mixup:** Create virtual samples by interpolating between pairs\n","3. **Feature Dropout:** Randomly zero out features during training\n","\n","**Why use augmentation?**\n","- Prevents overfitting\n","- Improves generalization\n","- Especially useful with small datasets\n","- Acts as regularization"],"metadata":{"id":"5uGIjxymQNnO"}},{"cell_type":"markdown","metadata":{"id":"MUPy5p-wQNnP"},"source":["## 6. Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3XSlrsIQNnP"},"outputs":[],"source":["# Load best model\n","model.load_state_dict(torch.load('best_model.pth'))\n","model.eval()\n","\n","# Make predictions\n","with torch.no_grad():\n","    y_train_pred_scaled = model(X_train_tensor.to(device)).cpu().numpy()\n","    y_test_pred_scaled = model(X_test_tensor.to(device)).cpu().numpy()\n","\n","# Inverse transform to original scale\n","y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled)\n","y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled)\n","\n","# Calculate metrics\n","train_r2 = r2_score(y_train, y_train_pred)\n","test_r2 = r2_score(y_test, y_test_pred)\n","test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n","test_mae = mean_absolute_error(y_test, y_test_pred)\n","\n","print(\"Model Performance:\")\n","print(\"=\" * 40)\n","print(f\"Training R²: {train_r2:.4f}\")\n","print(f\"Test R²: {test_r2:.4f}\")\n","print(f\"Test RMSE: {test_rmse:.4f}\")\n","print(f\"Test MAE: {test_mae:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOKsb90XQNnQ"},"outputs":[],"source":["# Actual vs Predicted plot\n","plt.figure(figsize=(10, 6))\n","plt.scatter(y_test, y_test_pred, alpha=0.5, s=50, edgecolors='black', linewidth=0.5)\n","\n","# Perfect prediction line\n","min_val = min(y_test.min(), y_test_pred.min())\n","max_val = max(y_test.max(), y_test_pred.max())\n","plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n","\n","plt.xlabel('Actual Log Solubility', fontsize=12)\n","plt.ylabel('Predicted Log Solubility', fontsize=12)\n","plt.title('PyTorch Neural Network: Actual vs Predicted', fontsize=14, fontweight='bold')\n","plt.grid(True, alpha=0.3)\n","plt.legend()\n","\n","# Add R² annotation\n","plt.text(0.05, 0.95, f'R² = {test_r2:.4f}\\nRMSE = {test_rmse:.4f}',\n","         transform=plt.gca().transAxes, fontsize=11, verticalalignment='top',\n","         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xQYp-ZBbQNnQ"},"source":["## 7. Comparing with Scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxEfkOh1QNnR"},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","# Train Random Forest for comparison\n","rf = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf.fit(X_train_scaled, y_train.ravel())\n","\n","y_test_pred_rf = rf.predict(X_test_scaled).reshape(-1, 1)\n","\n","# Metrics\n","rf_r2 = r2_score(y_test, y_test_pred_rf)\n","rf_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n","rf_mae = mean_absolute_error(y_test, y_test_pred_rf)\n","\n","# Compare\n","comparison = pd.DataFrame({\n","    'Model': ['PyTorch NN', 'Random Forest'],\n","    'Test R²': [test_r2, rf_r2],\n","    'Test RMSE': [test_rmse, rf_rmse],\n","    'Test MAE': [test_mae, rf_mae]\n","})\n","\n","print(\"\\nModel Comparison:\")\n","print(comparison.to_string(index=False))"]},{"cell_type":"markdown","metadata":{"id":"K606VyNYQNnR"},"source":["## 8. Understanding Neural Network Components\n","\n","### Key Concepts:\n","\n","1. **Layers**\n","   - `nn.Linear`: Fully connected layer\n","   - `nn.BatchNorm1d`: Normalizes layer inputs\n","   - `nn.Dropout`: Prevents overfitting\n","\n","2. **Activation Functions**\n","   - `ReLU`: Most common, fast\n","   - `Sigmoid`: For binary classification\n","   - `Tanh`: Centered around 0\n","\n","3. **Loss Functions**\n","   - `MSELoss`: Regression\n","   - `CrossEntropyLoss`: Classification\n","\n","4. **Optimizers**\n","   - `Adam`: Adaptive learning rate (most popular)\n","   - `SGD`: Stochastic Gradient Descent\n","   - `RMSprop`: Good for RNNs\n","\n","5. **Regularization**\n","   - Dropout: Randomly drop neurons\n","   - Batch Normalization: Stabilize training\n","   - Weight Decay: L2 regularization"]},{"cell_type":"markdown","metadata":{"id":"2C9lN3AoQNnS"},"source":["## 9. Summary and Best Practices\n","\n","### Key Takeaways:\n","\n","1. **Data Preparation**\n","   - Always scale features\n","   - Use DataLoader for efficient batch processing\n","   - Convert data to tensors\n","\n","2. **Model Architecture**\n","   - Start simple, add complexity if needed\n","   - Use batch normalization for stability\n","   - Add dropout to prevent overfitting\n","\n","3. **Training**\n","   - Monitor both training and validation loss\n","   - Use learning rate scheduling\n","   - Save the best model\n","   - Watch for overfitting\n","\n","4. **Evaluation**\n","   - Use appropriate metrics (R², RMSE, MAE)\n","   - Visualize predictions\n","   - Compare with baseline models\n","\n","### When to use Deep Learning?\n","\n","✓ **Use Neural Networks when:**\n","- Large dataset (>10,000 samples)\n","- Complex, non-linear relationships\n","- High-dimensional data\n","- Need feature learning\n","\n","✗ **Stick with traditional ML when:**\n","- Small dataset (<1,000 samples)\n","- Simple relationships\n","- Need interpretability\n","- Limited computational resources"]},{"cell_type":"markdown","metadata":{"id":"AwbiEVQHQNnS"},"source":["## 10. Exercise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vsbHSpOQNnU"},"outputs":[],"source":["# TODO: Experiment with the neural network\n","# 1. Try different architectures (more/fewer layers)\n","# 2. Experiment with dropout rates\n","# 3. Try different learning rates\n","# 4. Add more hidden layers\n","# 5. Compare performance\n","\n","# Your code here:\n"]},{"cell_type":"markdown","metadata":{"id":"o1cPjM18QNnU"},"source":["---\n","\n","## Resources\n","\n","- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n","- [PyTorch Tutorials](https://pytorch.org/tutorials/)\n","- [Deep Learning Book](https://www.deeplearningbook.org/)\n","- [Neural Networks Playground](https://playground.tensorflow.org/)\n","\n","**Next Notebook: HuggingFace Transformers for Chemistry**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[{"file_id":"https://github.com/jomaminoza/dost-ai-training/blob/main/notebooks/04_PyTorch_Basics.ipynb","timestamp":1764396955848}]}},"nbformat":4,"nbformat_minor":0}