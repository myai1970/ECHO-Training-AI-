{"cells":[{"cell_type":"markdown","metadata":{"id":"OPjOXcafP-n3"},"source":["# Machine Learning Classification Models\n","\n","**DOST-ITDI AI Training Workshop**  \n","**Day 1 - Session 3 (continued): Classification with Scikit-learn**\n","\n","---\n","\n","## Learning Objectives\n","1. Understand classification problems in chemistry\n","2. Implement classification algorithms\n","3. Evaluate models using classification metrics\n","4. Interpret confusion matrices and ROC curves\n","5. Handle imbalanced datasets\n","\n","## What is Classification?\n","\n","Classification predicts **categorical labels** (classes).\n","\n","**Chemistry Applications**:\n","- Drug activity (active/inactive)\n","- Toxicity prediction (toxic/non-toxic)\n","- Compound type classification\n","- Quality control (pass/fail)\n","- Reaction outcome (success/failure)"]},{"cell_type":"markdown","metadata":{"id":"zRJTMyLiP-n8"},"source":["## 1. Setup and Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRCSppztP-n9"},"outputs":[],"source":["# Install libraries\n","!pip install rdkit scikit-learn imbalanced-learn -q\n","\n","print(\"âœ“ Installation complete!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLJ-iGP6P-oA"},"outputs":[],"source":["# Import libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from rdkit import Chem\n","from rdkit.Chem import Descriptors\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Machine Learning\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Metrics\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score,\n","    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",")\n","\n","# Set random seed\n","np.random.seed(42)\n","\n","# Plotting\n","sns.set_style(\"whitegrid\")\n","plt.rcParams['figure.figsize'] = (10, 6)\n","\n","print(\"âœ“ Libraries imported successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysNmA8drP-oC"},"outputs":[],"source":["# Load BACE dataset (Blood-Brain Barrier Penetration)\n","# Binary classification: active (1) vs inactive (0)\n","url = \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/bace.csv\"\n","df = pd.read_csv(url)\n","\n","print(f\"Dataset shape: {df.shape}\")\n","print(f\"\\nColumns: {df.columns.tolist()}\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gbvxEtKP-oD"},"outputs":[],"source":["# Check target distribution - DETAILED ANALYSIS\n","print(\"=\"*60)\n","print(\"CLASS BALANCE ANALYSIS\")\n","print(\"=\"*60)\n","\n","# Absolute counts\n","print(\"\\n1. Class Counts:\")\n","class_counts = df['Class'].value_counts().sort_index()\n","print(class_counts)\n","\n","# Percentages\n","print(\"\\n2. Class Percentages:\")\n","class_percentages = df['Class'].value_counts(normalize=True).sort_index() * 100\n","for cls, pct in class_percentages.items():\n","    print(f\"   Class {cls}: {pct:.2f}%\")\n","\n","# Ratio\n","ratio = class_counts[0] / class_counts[1]\n","print(f\"\\n3. Majority to Minority Ratio: {ratio:.2f}:1\")\n","\n","# Interpretation guidance\n","print(f\"\\n4. Interpretation:\")\n","print(f\"   This dataset has a ratio of {ratio:.2f}:1\")\n","print(f\"   Generally considered balanced (close to 1:1)\")\n","print(f\"   Imbalance becomes a concern when ratio > 3:1 or 5:1\")\n","print(f\"   Severe imbalance: 10:1, 100:1, or higher\")\n","print(f\"   (Thresholds depend on problem domain and cost of errors)\")\n","\n","print(\"\\n\" + \"=\"*60)\n","\n","# Visualize\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Bar chart\n","class_counts.plot(kind='bar', ax=axes[0], color=['coral', 'steelblue'],\n","                  edgecolor='black', alpha=0.7)\n","axes[0].set_xlabel('Class (0=Inactive, 1=Active)', fontsize=12)\n","axes[0].set_ylabel('Count', fontsize=12)\n","axes[0].set_title('Class Distribution (Counts)', fontsize=14, fontweight='bold')\n","axes[0].set_xticklabels(['Inactive (0)', 'Active (1)'], rotation=0)\n","axes[0].grid(True, alpha=0.3, axis='y')\n","\n","# Add count labels on bars\n","for i, (idx, val) in enumerate(class_counts.items()):\n","    axes[0].text(i, val + 20, str(val), ha='center', fontsize=12, fontweight='bold')\n","\n","# Pie chart\n","axes[1].pie(class_counts.values, labels=['Inactive (0)', 'Active (1)'],\n","            autopct='%1.1f%%', startangle=90, colors=['coral', 'steelblue'],\n","            explode=[0.05, 0.05], shadow=True)\n","axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"3eSDW1EPP-oG"},"source":["## 2. Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLjZ2KzKP-oH"},"outputs":[],"source":["# Compute molecular descriptors\n","df['mol'] = df['mol'].apply(Chem.MolFromSmiles)\n","df = df[df['mol'].notna()].copy()\n","\n","# Calculate descriptors\n","df['MolWeight'] = df['mol'].apply(Descriptors.MolWt)\n","df['LogP'] = df['mol'].apply(Descriptors.MolLogP)\n","df['NumHDonors'] = df['mol'].apply(Descriptors.NumHDonors)\n","df['NumHAcceptors'] = df['mol'].apply(Descriptors.NumHAcceptors)\n","df['NumRotatableBonds'] = df['mol'].apply(Descriptors.NumRotatableBonds)\n","df['NumAromaticRings'] = df['mol'].apply(Descriptors.NumAromaticRings)\n","df['TPSA'] = df['mol'].apply(Descriptors.TPSA)\n","df['NumAtoms'] = df['mol'].apply(lambda x: x.GetNumAtoms())\n","\n","print(f\"Dataset after feature engineering: {df.shape[0]} molecules\")\n","print(\"\\nNew features:\")\n","print(df[['MolWeight', 'LogP', 'NumHDonors', 'NumHAcceptors']].head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNcd-avGP-oI"},"outputs":[],"source":["# Feature distribution by class\n","features_to_plot = ['MolWeight', 'LogP', 'NumHDonors', 'TPSA']\n","\n","fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n","axes = axes.ravel()\n","\n","for idx, feature in enumerate(features_to_plot):\n","    df.boxplot(column=feature, by='Class', ax=axes[idx], patch_artist=True)\n","    axes[idx].set_xlabel('Class', fontsize=11)\n","    axes[idx].set_ylabel(feature, fontsize=11)\n","    axes[idx].set_title(f'{feature} by Class', fontsize=12, fontweight='bold')\n","    axes[idx].get_figure().suptitle('')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ibt9HfLuP-oL"},"source":["## 3. Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUG_gVIvP-oM"},"outputs":[],"source":["# Select features and target\n","feature_columns = ['MolWeight', 'LogP', 'NumHDonors', 'NumHAcceptors',\n","                   'NumRotatableBonds', 'NumAromaticRings', 'TPSA', 'NumAtoms']\n","\n","X = df[feature_columns]\n","y = df['Class']\n","\n","print(f\"Features shape: {X.shape}\")\n","print(f\"Target shape: {y.shape}\")\n","print(f\"\\nClass distribution:\")\n","print(y.value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aynbfISZP-oN"},"outputs":[],"source":["# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y  # stratify maintains class balance\n",")\n","\n","print(\"Dataset Split:\")\n","print(f\"Training set: {X_train.shape[0]} samples\")\n","print(f\"Test set: {X_test.shape[0]} samples\")\n","print(f\"\\nTrain class distribution:\")\n","print(y_train.value_counts())\n","print(f\"\\nTest class distribution:\")\n","print(y_test.value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2iG-11isP-oO"},"outputs":[],"source":["# Feature scaling\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","print(\"âœ“ Feature scaling complete!\")"]},{"cell_type":"markdown","metadata":{"id":"FNIs6hyhP-oP"},"source":["## 4. Classification Models\n","\n","### 4.1 Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkqmGtB0P-oQ"},"outputs":[],"source":["# Train Logistic Regression\n","log_reg = LogisticRegression(random_state=42, max_iter=1000)\n","log_reg.fit(X_train_scaled, y_train)\n","\n","# Predictions\n","y_train_pred_lr = log_reg.predict(X_train_scaled)\n","y_test_pred_lr = log_reg.predict(X_test_scaled)\n","y_test_proba_lr = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n","\n","# Evaluate\n","print(\"Logistic Regression Results:\")\n","print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_lr):.4f}\")\n","print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred_lr):.4f}\")\n","print(f\"Test Precision: {precision_score(y_test, y_test_pred_lr):.4f}\")\n","print(f\"Test Recall: {recall_score(y_test, y_test_pred_lr):.4f}\")\n","print(f\"Test F1-Score: {f1_score(y_test, y_test_pred_lr):.4f}\")\n","print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_lr):.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"x-NSCFUhP-oR"},"source":["### 4.2 Random Forest Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHNsj35LP-oR"},"outputs":[],"source":["# Train Random Forest\n","rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n","rf_clf.fit(X_train_scaled, y_train)\n","\n","# Predictions\n","y_train_pred_rf = rf_clf.predict(X_train_scaled)\n","y_test_pred_rf = rf_clf.predict(X_test_scaled)\n","y_test_proba_rf = rf_clf.predict_proba(X_test_scaled)[:, 1]\n","\n","# Evaluate\n","print(\"Random Forest Results:\")\n","print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_rf):.4f}\")\n","print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred_rf):.4f}\")\n","print(f\"Test Precision: {precision_score(y_test, y_test_pred_rf):.4f}\")\n","print(f\"Test Recall: {recall_score(y_test, y_test_pred_rf):.4f}\")\n","print(f\"Test F1-Score: {f1_score(y_test, y_test_pred_rf):.4f}\")\n","print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_rf):.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxXQSTuPP-oS"},"outputs":[],"source":["# Feature importance\n","feature_imp = pd.DataFrame({\n","    'Feature': feature_columns,\n","    'Importance': rf_clf.feature_importances_\n","}).sort_values('Importance', ascending=False)\n","\n","plt.figure(figsize=(10, 6))\n","plt.barh(feature_imp['Feature'], feature_imp['Importance'],\n","         color='steelblue', alpha=0.7, edgecolor='black')\n","plt.xlabel('Importance Score', fontsize=12)\n","plt.title('Random Forest: Feature Importance', fontsize=14, fontweight='bold')\n","plt.grid(True, alpha=0.3, axis='x')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6A5cc0IuP-oS"},"source":["### 4.3 Gradient Boosting Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATF-Hw8cP-oT"},"outputs":[],"source":["# Train Gradient Boosting\n","gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","gb_clf.fit(X_train_scaled, y_train)\n","\n","# Predictions\n","y_train_pred_gb = gb_clf.predict(X_train_scaled)\n","y_test_pred_gb = gb_clf.predict(X_test_scaled)\n","y_test_proba_gb = gb_clf.predict_proba(X_test_scaled)[:, 1]\n","\n","# Evaluate\n","print(\"Gradient Boosting Results:\")\n","print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_gb):.4f}\")\n","print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred_gb):.4f}\")\n","print(f\"Test Precision: {precision_score(y_test, y_test_pred_gb):.4f}\")\n","print(f\"Test Recall: {recall_score(y_test, y_test_pred_gb):.4f}\")\n","print(f\"Test F1-Score: {f1_score(y_test, y_test_pred_gb):.4f}\")\n","print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_gb):.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"jHKxFbAwP-oT"},"source":["## 5. Model Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05ExiKLZP-oU"},"outputs":[],"source":["# Compile results\n","results = pd.DataFrame({\n","    'Model': ['Logistic Regression', 'Random Forest', 'Gradient Boosting'],\n","    'Accuracy': [\n","        accuracy_score(y_test, y_test_pred_lr),\n","        accuracy_score(y_test, y_test_pred_rf),\n","        accuracy_score(y_test, y_test_pred_gb)\n","    ],\n","    'Precision': [\n","        precision_score(y_test, y_test_pred_lr),\n","        precision_score(y_test, y_test_pred_rf),\n","        precision_score(y_test, y_test_pred_gb)\n","    ],\n","    'Recall': [\n","        recall_score(y_test, y_test_pred_lr),\n","        recall_score(y_test, y_test_pred_rf),\n","        recall_score(y_test, y_test_pred_gb)\n","    ],\n","    'F1-Score': [\n","        f1_score(y_test, y_test_pred_lr),\n","        f1_score(y_test, y_test_pred_rf),\n","        f1_score(y_test, y_test_pred_gb)\n","    ],\n","    'ROC-AUC': [\n","        roc_auc_score(y_test, y_test_proba_lr),\n","        roc_auc_score(y_test, y_test_proba_rf),\n","        roc_auc_score(y_test, y_test_proba_gb)\n","    ]\n","})\n","\n","print(\"\\nModel Performance Comparison:\")\n","print(results.to_string(index=False))\n","\n","# Best model\n","best_model_idx = results['ROC-AUC'].idxmax()\n","print(f\"\\nðŸ† Best Model: {results.loc[best_model_idx, 'Model']}\")\n","print(f\"   ROC-AUC: {results.loc[best_model_idx, 'ROC-AUC']:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vILAlFfNP-oV"},"outputs":[],"source":["# Visualize comparison\n","fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","\n","# Metrics comparison\n","metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n","x = np.arange(len(results))\n","width = 0.2\n","\n","for i, metric in enumerate(metrics_to_plot):\n","    axes[0].bar(x + i*width, results[metric], width, label=metric, alpha=0.8)\n","\n","axes[0].set_xlabel('Model', fontsize=12)\n","axes[0].set_ylabel('Score', fontsize=12)\n","axes[0].set_title('Classification Metrics Comparison', fontsize=13, fontweight='bold')\n","axes[0].set_xticks(x + width * 1.5)\n","axes[0].set_xticklabels(results['Model'], rotation=45, ha='right')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3, axis='y')\n","\n","# ROC-AUC comparison\n","axes[1].bar(results['Model'], results['ROC-AUC'],\n","            color='coral', alpha=0.8, edgecolor='black')\n","axes[1].set_xlabel('Model', fontsize=12)\n","axes[1].set_ylabel('ROC-AUC Score', fontsize=12)\n","axes[1].set_title('ROC-AUC Comparison', fontsize=13, fontweight='bold')\n","axes[1].set_xticklabels(results['Model'], rotation=45, ha='right')\n","axes[1].set_ylim([0.5, 1.0])\n","axes[1].grid(True, alpha=0.3, axis='y')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Ur882FcGP-oW"},"source":["## 6. Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPMNKa8CP-oW"},"outputs":[],"source":["# Confusion matrices for all models\n","fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n","\n","predictions = [\n","    ('Logistic Regression', y_test_pred_lr),\n","    ('Random Forest', y_test_pred_rf),\n","    ('Gradient Boosting', y_test_pred_gb)\n","]\n","\n","for idx, (name, y_pred) in enumerate(predictions):\n","    cm = confusion_matrix(y_test, y_pred)\n","\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],\n","                xticklabels=['Inactive', 'Active'],\n","                yticklabels=['Inactive', 'Active'])\n","    axes[idx].set_xlabel('Predicted', fontsize=11)\n","    axes[idx].set_ylabel('Actual', fontsize=11)\n","    axes[idx].set_title(f'{name}', fontsize=12, fontweight='bold')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVbfx8xpP-oX"},"outputs":[],"source":["# Detailed confusion matrix explanation for Random Forest\n","cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n","\n","tn, fp, fn, tp = cm_rf.ravel()\n","\n","print(\"Confusion Matrix Breakdown (Random Forest):\")\n","print(f\"True Negatives (TN): {tn} - Correctly predicted as Inactive\")\n","print(f\"False Positives (FP): {fp} - Incorrectly predicted as Active\")\n","print(f\"False Negatives (FN): {fn} - Incorrectly predicted as Inactive\")\n","print(f\"True Positives (TP): {tp} - Correctly predicted as Active\")\n","print(f\"\\nTotal predictions: {tn + fp + fn + tp}\")\n","print(f\"Correct predictions: {tn + tp} ({(tn + tp)/(tn + fp + fn + tp)*100:.2f}%)\")\n","print(f\"Incorrect predictions: {fp + fn} ({(fp + fn)/(tn + fp + fn + tp)*100:.2f}%)\")"]},{"cell_type":"markdown","metadata":{"id":"jurwtOf2P-oY"},"source":["## 7. ROC Curve Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BNbA8_3GP-oY"},"outputs":[],"source":["# Plot ROC curves for all models\n","plt.figure(figsize=(10, 8))\n","\n","models_proba = [\n","    ('Logistic Regression', y_test_proba_lr),\n","    ('Random Forest', y_test_proba_rf),\n","    ('Gradient Boosting', y_test_proba_gb)\n","]\n","\n","for name, y_proba in models_proba:\n","    fpr, tpr, _ = roc_curve(y_test, y_proba)\n","    auc = roc_auc_score(y_test, y_proba)\n","    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {auc:.3f})')\n","\n","# Random classifier line\n","plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5)')\n","\n","plt.xlabel('False Positive Rate', fontsize=12)\n","plt.ylabel('True Positive Rate', fontsize=12)\n","plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n","plt.legend(loc='lower right', fontsize=11)\n","plt.grid(True, alpha=0.3)\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"eHBhrKFHP-oa"},"source":["## 8. Classification Report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55iEflUjP-oa"},"outputs":[],"source":["# Detailed classification report for Random Forest\n","print(\"Classification Report (Random Forest):\")\n","print(\"=\" * 60)\n","print(classification_report(y_test, y_test_pred_rf,\n","                           target_names=['Inactive (0)', 'Active (1)']))"]},{"cell_type":"markdown","metadata":{"id":"rdSGeHvPP-ob"},"source":["## 9. Understanding Classification Metrics\n","\n","### Key Metrics Explained:\n","\n","1. **Accuracy**: (TP + TN) / Total\n","   - Overall correctness\n","   - Can be misleading with imbalanced data\n","\n","2. **Precision**: TP / (TP + FP)\n","   - Of all positive predictions, how many are correct?\n","   - Important when false positives are costly\n","\n","3. **Recall (Sensitivity)**: TP / (TP + FN)\n","   - Of all actual positives, how many did we find?\n","   - Important when false negatives are costly\n","\n","4. **F1-Score**: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n","   - Harmonic mean of precision and recall\n","   - Balanced measure\n","\n","5. **ROC-AUC**: Area Under the ROC Curve\n","   - Overall model discrimination ability\n","   - Threshold-independent\n","\n","### When to use which metric?\n","\n","- **Drug Discovery**: High recall (don't miss active compounds)\n","- **Toxicity Screening**: High precision (avoid false alarms)\n","- **Quality Control**: Balance with F1-score\n","- **Model Comparison**: ROC-AUC"]},{"cell_type":"markdown","metadata":{"id":"_ROIOT8iP-ob"},"source":["## 10. Prediction Threshold Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUP5iu9-P-oc"},"outputs":[],"source":["# Try different thresholds\n","thresholds = np.arange(0.1, 1.0, 0.05)\n","precision_scores = []\n","recall_scores = []\n","f1_scores = []\n","\n","for threshold in thresholds:\n","    y_pred_threshold = (y_test_proba_rf >= threshold).astype(int)\n","    precision_scores.append(precision_score(y_test, y_pred_threshold))\n","    recall_scores.append(recall_score(y_test, y_pred_threshold))\n","    f1_scores.append(f1_score(y_test, y_pred_threshold))\n","\n","# Plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(thresholds, precision_scores, 'o-', label='Precision', linewidth=2)\n","plt.plot(thresholds, recall_scores, 's-', label='Recall', linewidth=2)\n","plt.plot(thresholds, f1_scores, '^-', label='F1-Score', linewidth=2)\n","plt.axvline(x=0.5, color='gray', linestyle='--', linewidth=1, label='Default (0.5)')\n","plt.xlabel('Classification Threshold', fontsize=12)\n","plt.ylabel('Score', fontsize=12)\n","plt.title('Metrics vs Classification Threshold', fontsize=14, fontweight='bold')\n","plt.legend(fontsize=11)\n","plt.grid(True, alpha=0.3)\n","plt.show()\n","\n","# Find optimal threshold for F1\n","optimal_idx = np.argmax(f1_scores)\n","optimal_threshold = thresholds[optimal_idx]\n","print(f\"\\nOptimal threshold for F1-Score: {optimal_threshold:.2f}\")\n","print(f\"F1-Score at optimal threshold: {f1_scores[optimal_idx]:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"RXRQDgXZP-oc"},"source":["## 11. Making Predictions"]},{"cell_type":"markdown","metadata":{"id":"T4wqgS0cP-oc"},"source":["### 11.5.4 Summary: Molecular vs Data Augmentation\n","\n","**Two Levels of Augmentation:**\n","\n","| Aspect | Molecular Augmentation | Data Augmentation (SMOTE) |\n","|--------|------------------------|---------------------------|\n","| **What it does** | Varies molecular representation | Creates synthetic feature vectors |\n","| **Level** | Molecule-level | Feature-level |\n","| **Examples** | SMILES enumeration, noise | Interpolation between samples |\n","| **Preserves** | Chemical identity | Statistical distribution |\n","| **Use case** | Deep learning on SMILES/graphs | Traditional ML on descriptors |\n","\n","**Connection to Computer Vision:**\n","\n","Just like images can be augmented:\n","\n","| Image Augmentation | Molecular Augmentation |\n","|--------------------|------------------------|\n","| Rotation | SMILES enumeration (different atom ordering) |\n","| Flipping | Mirror image (stereoisomers) |\n","| Noise/Blur | Descriptor noise injection |\n","| Color jitter | Conformer variations |\n","| Cropping | Substructure extraction |\n","\n","**When to Use What:**\n","\n","1. **SMILES Enumeration**\n","   - Training neural networks on SMILES\n","   - Graph neural networks\n","   - Transformer models (ChemBERTa)\n","\n","2. **Noise Injection**\n","   - Traditional ML (Random Forest, SVM)\n","   - Simulating measurement uncertainty\n","   - Small datasets\n","\n","3. **SMOTE**\n","   - Class imbalance (covered in Section 12)\n","   - Feature-space augmentation\n","   - Works with any classifier\n","\n","**Best Practice:**\n","- Combine techniques! Use SMOTE for class balance + noise injection for robustness\n","- For deep learning: SMILES enumeration during training\n","- For traditional ML: Noise injection + SMOTE\n","\n","**Connection to Other Notebooks:**\n","- **Notebook 06 (Computer Vision)**: Image augmentation techniques\n","- **Notebook 04 (PyTorch)**: Data augmentation in training loops\n","- **Notebook 04b (HuggingFace)**: ChemBERTa uses SMILES enumeration\n","\n","---\n","\n","Now let's explore data-level techniques for handling imbalanced datasets!"]},{"cell_type":"code","source":["# Create Imbalanced Baseline Dataset\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","print(\"=\"*60)\n","print(\"CREATING IMBALANCED BASELINE DATASET\")\n","print(\"=\"*60)\n","\n","print(\"\\nOriginal Training Data:\")\n","print(f\"Training set size: {X_train_scaled.shape[0]}\")\n","class_counts_before = y_train.value_counts().sort_index()\n","print(f\"Class distribution:\\n{class_counts_before}\")\n","ratio_before = class_counts_before[0] / class_counts_before[1]\n","print(f\"Ratio: {ratio_before:.2f}:1 (relatively balanced)\")\n","\n","# Apply under-sampling - Remove 50% of majority class (Class 0)\n","# This creates an imbalanced dataset for demonstrating handling techniques\n","majority_class = class_counts_before.idxmax()\n","majority_count = class_counts_before[majority_class]\n","target_majority_count = int(majority_count * 0.5)\n","\n","print(f\"\\nCreating Imbalance: Remove 50% of Class {majority_class}\")\n","print(f\"  Original: {majority_count} samples\")\n","print(f\"  Target: {target_majority_count} samples\")\n","print(f\"  Removing: {majority_count - target_majority_count} samples\")\n","\n","rus = RandomUnderSampler(sampling_strategy={majority_class: target_majority_count}, random_state=42)\n","X_train_rus, y_train_rus = rus.fit_resample(X_train_scaled, y_train)\n","\n","print(f\"\\nImbalanced Baseline Dataset Created:\")\n","print(f\"Training set size: {X_train_rus.shape[0]}\")\n","class_counts_after = pd.Series(y_train_rus).value_counts().sort_index()\n","print(f\"Class distribution:\\n{class_counts_after}\")\n","ratio_after = class_counts_after[1] / class_counts_after[0]\n","print(f\"Ratio: {ratio_after:.2f}:1 (Class 1 is now majority)\")\n","print(f\"\\nThis imbalanced dataset will be used to demonstrate\")\n","print(f\"different techniques for handling class imbalance.\")\n","print(\"=\"*60)"],"metadata":{"id":"20vlZirnP-od"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compare all techniques\n","print(\"\\n\" + \"=\"*60)\n","print(\"COMPARISON OF ALL TECHNIQUES\")\n","print(\"=\"*60)\n","\n","print(\"\\nAll models trained on same imbalanced baseline (328:553)\")\n","print(\"Evaluated on same test set\\n\")\n","\n","techniques = {\n","    'No Handling': y_pred_default,\n","    'Class Weights': y_pred_balanced,\n","    'SMOTE': y_pred_smote\n","}\n","\n","comparison_results = []\n","\n","for name, y_pred in techniques.items():\n","    comparison_results.append({\n","        'Technique': name,\n","        'Accuracy': accuracy_score(y_test, y_pred),\n","        'Precision': precision_score(y_test, y_pred),\n","        'Recall': recall_score(y_test, y_pred),\n","        'F1-Score': f1_score(y_test, y_pred)\n","    })\n","\n","comparison_df = pd.DataFrame(comparison_results)\n","print(\"\\nComparison of Imbalance Handling Techniques:\")\n","print(\"=\"*80)\n","print(comparison_df.to_string(index=False))\n","\n","# Visualize\n","fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","\n","# Precision vs Recall scatter\n","axes[0].scatter(comparison_df['Recall'], comparison_df['Precision'],\n","               s=200, alpha=0.6, c=range(len(comparison_df)),\n","               cmap='viridis', edgecolors='black', linewidth=2)\n","for idx, row in comparison_df.iterrows():\n","    axes[0].annotate(row['Technique'],\n","                    (row['Recall'], row['Precision']),\n","                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n","axes[0].set_xlabel('Recall', fontsize=12)\n","axes[0].set_ylabel('Precision', fontsize=12)\n","axes[0].set_title('Precision vs Recall Trade-off', fontsize=13, fontweight='bold')\n","axes[0].grid(True, alpha=0.3)\n","\n","# F1-Score comparison\n","axes[1].barh(comparison_df['Technique'], comparison_df['F1-Score'],\n","            color='steelblue', alpha=0.7, edgecolor='black')\n","axes[1].set_xlabel('F1-Score', fontsize=12)\n","axes[1].set_title('F1-Score by Technique', fontsize=13, fontweight='bold')\n","axes[1].grid(True, alpha=0.3, axis='x')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\nKey Insights:\")\n","print(\"- All techniques trained on same imbalanced baseline (328:553 = 1.69:1 ratio)\")\n","print(\"- SMOTE and Class Weights help handle the imbalance\")\n","print(\"- No Handling shows baseline performance without any technique\")\n","print(\"- Compare Recall for minority class (Class 0) - higher is better\")\n","print(\"- For more severe imbalance (10:1, 100:1), differences would be larger\")\n","print(\"=\"*60)"],"metadata":{"id":"lqjOH__HP-oe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 12.5 Summary: Which Technique to Use?\n","\n","| Technique | Pros | Cons | Best For |\n","|-----------|------|------|----------|\n","| **Class Weights** | Simple, no data change, fast | Limited for severe imbalance | First approach, moderate imbalance |\n","| **SMOTE** | Generates new data, maintains info | Can create noise | Good minority samples, SVM/NN |\n","| **Under-sampling** | Fast, reduces size | Loses information | Large datasets, time constraints |\n","\n","**General Recommendations:**\n","\n","1. **Start Simple:** Try class weights first\n","2. **Moderate Imbalance (3:1 to 10:1):** Class weights or SMOTE\n","3. **Severe Imbalance (>10:1):** SMOTE or combination methods\n","4. **Huge Dataset:** Under-sampling acceptable\n","5. **Always:** Use stratified cross-validation\n","6. **Metrics:** Focus on F1, precision, recall - not accuracy!\n","\n","**For Chemistry/Drug Discovery:**\n","- **Toxicity screening:** Prioritize recall (don't miss toxic compounds)\n","- **Hit identification:** Balance precision/recall with F1-score\n","- **Rare effects:** Definitely need resampling techniques"],"metadata":{"id":"rc0P0II6P-oe"}},{"cell_type":"markdown","source":["### 12.4 Comparing All Techniques"],"metadata":{"id":"Hrv40TBlP-of"}},{"cell_type":"code","source":["# Technique 1: SMOTE (Synthetic Minority Over-sampling Technique)\n","from imblearn.over_sampling import SMOTE\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"TECHNIQUE 1: SMOTE\")\n","print(\"=\"*60)\n","\n","print(\"\\nStarting from Imbalanced Baseline:\")\n","print(f\"Training set size: {X_train_rus.shape[0]}\")\n","baseline_counts = pd.Series(y_train_rus).value_counts().sort_index()\n","print(f\"Class distribution:\\n{baseline_counts}\")\n","print(f\"Ratio: {baseline_counts[1]/baseline_counts[0]:.2f}:1 (imbalanced)\")\n","\n","# Apply SMOTE to imbalanced baseline\n","print(\"\\nApplying SMOTE...\")\n","smote = SMOTE(random_state=42)\n","X_train_smote, y_train_smote = smote.fit_resample(X_train_rus, y_train_rus)\n","\n","print(f\"\\nAfter SMOTE:\")\n","print(f\"Training set size: {X_train_smote.shape[0]}\")\n","print(f\"Class distribution:\\n{pd.Series(y_train_smote).value_counts()}\")\n","print(f\"Ratio: 1:1 (perfectly balanced)\")\n","\n","# Train model on SMOTE data\n","rf_smote = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_smote.fit(X_train_smote, y_train_smote)\n","y_pred_smote = rf_smote.predict(X_test_scaled)\n","\n","print(\"\\nModel Performance with SMOTE:\")\n","print(classification_report(y_test, y_pred_smote,\n","                           target_names=['Inactive', 'Active']))\n","\n","# Visualize SMOTE effect (2D projection)\n","from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components=2)\n","X_baseline_pca = pca.fit_transform(X_train_rus)\n","X_smote_pca = pca.transform(X_train_smote)\n","\n","fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","\n","# Imbalanced Baseline\n","for class_val in [0, 1]:\n","    mask = y_train_rus == class_val\n","    axes[0].scatter(X_baseline_pca[mask, 0], X_baseline_pca[mask, 1],\n","                   label=f'Class {class_val}', alpha=0.6, s=30)\n","axes[0].set_title('Imbalanced Baseline (328:553)', fontsize=13, fontweight='bold')\n","axes[0].set_xlabel('First Principal Component')\n","axes[0].set_ylabel('Second Principal Component')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","# After SMOTE\n","for class_val in [0, 1]:\n","    mask = y_train_smote == class_val\n","    axes[1].scatter(X_smote_pca[mask, 0], X_smote_pca[mask, 1],\n","                   label=f'Class {class_val}', alpha=0.6, s=30)\n","axes[1].set_title('After SMOTE (Balanced)', fontsize=13, fontweight='bold')\n","axes[1].set_xlabel('First Principal Component')\n","axes[1].set_ylabel('Second Principal Component')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","print(\"=\"*60)"],"metadata":{"id":"oCD-2V7FP-of"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 12.3 Technique 3: Random Under-sampling\n","\n","Remove samples from majority class to balance with minority class.\n","\n","**How it works:**\n","- Randomly remove majority class samples\n","- Until both classes have similar size\n","\n","**When to use:**\n","- Very large datasets where losing data is acceptable\n","- When training time is critical\n","- When majority class has redundant information\n","\n","**Caution:**\n","- Loses potentially useful information\n","- May hurt model performance\n","- Only use if you have lots of majority samples"],"metadata":{"id":"Uqb-aseeP-og"}},{"cell_type":"markdown","source":["### Technique 1: SMOTE (Synthetic Minority Over-sampling Technique)\n","\n","Generate synthetic samples of minority class by interpolating between existing samples.\n","\n","**How it works:**\n","1. Select a minority sample\n","2. Find its k nearest neighbors\n","3. Create new sample between original and neighbor\n","4. Repeat until classes balanced\n","\n","**When to use:**\n","- Moderate to severe imbalance\n","- When you need more training data\n","- Works well with SVM, Neural Networks\n","\n","**In our demo:**\n","- Starts with imbalanced baseline (328:553)\n","- SMOTE creates synthetic samples for Class 0 (minority)\n","- Results in perfectly balanced dataset (553:553)"],"metadata":{"id":"9yM50sNdP-oh"}},{"cell_type":"markdown","source":["### 12.2 Technique 2: SMOTE (Synthetic Minority Over-sampling Technique)\n","\n","Generate synthetic samples of minority class by interpolating between existing samples.\n","\n","**How it works:**\n","1. Select a minority sample\n","2. Find its k nearest neighbors\n","3. Create new sample between original and neighbor\n","4. Repeat until classes balanced\n","\n","**When to use:**\n","- Moderate to severe imbalance\n","- When you need more training data\n","- Works well with SVM, Neural Networks\n","\n","**Caution:**\n","- May create synthetic samples in noisy regions\n","- Can lead to overfitting if overused"],"metadata":{"id":"vm7yFKkcP-oh"}},{"cell_type":"markdown","source":["### Technique 2 & 3: No Handling (Baseline) and Class Weights\n","\n","**Technique 2: No Handling (Baseline)**\n","- Train model directly on imbalanced data\n","- No special techniques applied\n","- Shows baseline performance for comparison\n","\n","**Technique 3: Class Weights**\n","Automatically adjust model to penalize misclassification of minority class more heavily.\n","\n","**How it works:**\n","- Assigns higher weight to minority class\n","- Model pays more attention to minority samples during training\n","- No data modification needed\n","- Formula: weight = n_samples / (n_classes * class_count)\n","\n","**When to use:**\n","- First approach to try\n","- Moderate imbalance (up to 5:1)\n","- When you want to keep original data\n","\n","**In our demo:**\n","- Both techniques use imbalanced baseline (328:553)\n","- Class weights: Class 0 gets ~1.34x weight, Class 1 gets ~0.80x weight\n","- Minority class errors penalized more during training"],"metadata":{"id":"4aH5aJJ8P-oh"}},{"cell_type":"markdown","source":["### 12.1 Technique 1: Class Weights\n","\n","Automatically adjust model to penalize misclassification of minority class more heavily.\n","\n","**How it works:**\n","- Assigns higher weight to minority class\n","- Model pays more attention to minority samples during training\n","- No data modification needed\n","\n","**When to use:**\n","- First approach to try\n","- Moderate imbalance (up to 5:1)\n","- When you want to keep original data"],"metadata":{"id":"z43yD7NCP-oi"}},{"cell_type":"markdown","source":["## 12. Handling Imbalanced Datasets (Reference Guide)\n","\n","### Note on This Dataset\n","\n","Our BACE dataset is relatively balanced (54% vs 46%). However, in real-world chemistry applications, you'll often encounter imbalanced datasets:\n","\n","**Common Imbalanced Scenarios:**\n","- Toxicity screening: 95% non-toxic, 5% toxic\n","- Rare adverse effects: 99% safe, 1% adverse\n","- Hit identification: 98% inactive, 2% active\n","- Quality control: 97% pass, 3% fail\n","\n","This section shows techniques for handling such cases.\n","\n","### What is Class Imbalance?\n","\n","Class imbalance occurs when one class has significantly more samples than another.\n","\n","**Problems it causes:**\n","- Model biased toward majority class\n","- Can achieve high accuracy by always predicting majority\n","- Poor detection of minority class (often the important one!)\n","- Misleading accuracy metric"],"metadata":{"id":"7oHodWfQP-oi"}},{"cell_type":"markdown","source":["## 12. Handling Imbalanced Datasets\n","\n","### What is Class Imbalance?\n","\n","Class imbalance occurs when one class has significantly more samples than another.\n","\n","**Example:**\n","- Toxicity: 95% non-toxic, 5% toxic\n","- Rare diseases: 99% negative, 1% positive\n","- Quality control: 98% pass, 2% fail\n","\n","**Problems:**\n","- Model biased toward majority class\n","- High accuracy but poor minority class detection\n","- Misleading performance metrics"],"metadata":{"id":"PTend2WPP-oj"}},{"cell_type":"markdown","metadata":{"id":"kLFQLt5yP-oj"},"source":["## 12. Summary and Best Practices\n","\n","### Key Takeaways:\n","\n","1. **Data Preparation**\n","   - Check class balance\n","   - Use stratified split\n","   - Scale features appropriately\n","\n","2. **Model Selection**\n","   - Start simple (Logistic Regression)\n","   - Try ensemble methods (Random Forest, Gradient Boosting)\n","   - Compare multiple models\n","\n","3. **Evaluation**\n","   - Don't rely on accuracy alone\n","   - Use confusion matrix\n","   - Consider precision/recall trade-off\n","   - Use ROC-AUC for overall performance\n","\n","4. **Chemistry-Specific Considerations**\n","   - Feature importance reveals key molecular properties\n","   - Threshold optimization depends on use case\n","   - Consider cost of false positives vs false negatives\n","\n","### Next Steps:\n","- Deep Learning with PyTorch\n","- Neural networks for molecular data\n","- Transfer learning with transformers"]},{"cell_type":"markdown","metadata":{"id":"UqKfxhpWP-ok"},"source":["## 13. Exercise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fS_nS5vwP-ok"},"outputs":[],"source":["# TODO: Try the following:\n","# 1. Train a Support Vector Machine (SVC) classifier\n","# 2. Compare its performance with the models above\n","# 3. Try adjusting the classification threshold\n","# 4. Create a confusion matrix for your model\n","\n","# Your code here:\n"]},{"cell_type":"markdown","metadata":{"id":"KWXJuoznP-ol"},"source":["---\n","\n","## Resources\n","\n","- [Scikit-learn Classification Guide](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n","- [Understanding ROC Curves](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n","- [Precision vs Recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n","\n","**Next Notebook: Deep Learning with PyTorch**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[{"file_id":"https://github.com/jomaminoza/dost-ai-training/blob/main/notebooks/03_Classification.ipynb","timestamp":1764396900638}]}},"nbformat":4,"nbformat_minor":0}